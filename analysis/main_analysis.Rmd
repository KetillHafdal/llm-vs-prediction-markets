---
title: "Analysis_data_new"
author: "Ketill"
date: "2025-11-03"
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

# Chapter 1 – Setup and Data Loading

This chapter sets up the R environment, imports the merged dataset, standardises column names and dates, and runs basic integrity checks needed for later analysis.

It also acknowledges that **end_date**, **volume_num**, and **liquidity_num** will be NA before **2025-08-08**, market-level metadata collection started on 2025-08-08; earlier rows are expected to have NA in these fields.

```{r setup, include=FALSE}
knitr::opts_chunk$set(
echo = TRUE, # show code in knitted output
message = FALSE, # hide package loading messages
warning = FALSE # hide warnings to avoid clutter
)
```

```{r TU DELFT PALETTE, include=FALSE}

#Primary (for titles and axis names)

tud_blue <- "#00A6D6"
tud_black <- "#000000"
tud_white <- "#FFFFFF"

#Secondary (for fills/lines/series)

tud_sec <- c(
"Dark blue" = "#0C2340",
"Turquoise" = "#00B8C8",
"Royal blue" = "#0076C2",
"Light purple" = "#6F1D77",
"Pink" = "#EF60A3",
"Burgundy" = "#A50034",
"Red" = "#E03C31",
"Orange" = "#EC6842",
"Yellow" = "#FFB81C",
"Green" = "#6CC24A",
"Forrest green" = "#009B77",
"Dark Grey" = "#5C5C5C"
)
```

```{r TU DELFT THEME, include=FALSE}
#Use TU Delft Blue for plot/axis/legend titles.
#Use black for tick labels and legend text.
#Keep clean white background for publications.

theme_tud <- function(base_size = 12, base_family = "") {
ggplot2::theme_minimal(base_size = base_size, base_family = base_family) +
ggplot2::theme(
# Background
plot.background = ggplot2::element_rect(fill = tud_white, color = NA),
panel.background = ggplot2::element_rect(fill = tud_white, color = NA),
panel.grid.minor = ggplot2::element_blank(),
panel.grid.major = ggplot2::element_line(color = "#E5E5E5"),

  # Titles in TU Delft Blue
  plot.title    = ggplot2::element_text(color = tud_blue, face = "bold", size = base_size + 2),
  plot.subtitle = ggplot2::element_text(color = tud_blue, size = base_size),
  axis.title.x  = ggplot2::element_text(color = tud_blue, face = "bold"),
  axis.title.y  = ggplot2::element_text(color = tud_blue, face = "bold"),
  legend.title  = ggplot2::element_text(color = tud_blue, face = "bold"),
  
  # Labels in black
  axis.text.x = ggplot2::element_text(color = tud_black),
  axis.text.y = ggplot2::element_text(color = tud_black),
  legend.text = ggplot2::element_text(color = tud_black),
  strip.text  = ggplot2::element_text(color = tud_black, face = "bold")
)

}
```

```{r STANDARD SCALES, include=FALSE}
# --- STANDARD SCALES (clean, non-duplicated) ---

# 1) Forecast Source (use fixed, human-readable labels if possible)
# If your data uses "brier_llm"/"brier_market", either:
#   a) keep the keys below and map your legend labels later, or
#   b) standardize the data to "LLM"/"Polymarket" and use those keys here.
curve_palette <- c(
  "LLM"        = tud_sec["Orange"],     # LLM forecast curve
  "Polymarket" = tud_sec["Royal blue"]  # Market forecast curve
)

# 2) Prompt Type (fixed mapping, as per thesis rules)
prompt_palette <- c(
  "Zero-shot"         = tud_sec["Green"],
  "Chain-of-Thought"  = tud_sec["Burgundy"]
)

# --- Scale helpers (no duplicates) ---
scale_fill_tud_prompt  <- function(...) ggplot2::scale_fill_manual(values = prompt_palette, drop = FALSE, ...)
scale_color_tud_prompt <- function(...) ggplot2::scale_color_manual(values = prompt_palette, drop = FALSE, ...)

scale_fill_tud_curve   <- function(...) ggplot2::scale_fill_manual(values = curve_palette, drop = FALSE, ...)
scale_color_tud_curve  <- function(...) ggplot2::scale_color_manual(values = curve_palette, drop = FALSE, ...)

# --- Axis helper for dense labels ---
rotate_x45 <- function() ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))

# --- Optional safety check: NA by design before 2025-08-08 ---
assert_na_before_cutoff <- function(df, date_col = "scrape_date",
                                    cols = c("end_date","volume_num","liquidity_num"),
                                    cutoff = as.Date("2025-08-08")) {
  if (!all(c(date_col, cols) %in% names(df))) return(invisible(TRUE))
  early <- df[[date_col]] < cutoff
  if (any(early, na.rm = TRUE)) {
    bad <- vapply(cols, function(cl) any(!is.na(df[[cl]][early])), logical(1))
    if (any(bad)) {
      stop(sprintf("Expected NAs before %s for: %s",
                   cutoff, paste(names(bad)[bad], collapse = ", ")))
    }
  }
  invisible(TRUE)
}
```

## 1.2 Load and verify required packages

This sub chapter loads and verifies the required packages used for data analysis and presentation later in the report.

We use the following key packages throughout the analysis:

| Package | Purpose |
|:-----------------------------------|:-----------------------------------|
| **tidyverse** | Core toolkit for data wrangling and visualization (includes `dplyr`, `ggplot2`, `readr`, etc.) |
| **dplyr** | Grammar of data manipulation (filtering, grouping, summarizing, etc.) |
| **ggplot2** | Grammar of data visualization used for all plots |
| **janitor** | Cleaning and standardizing column names (`clean_names()`) |
| **lubridate** | Handling and converting dates and times |
| **kableExtra** | Creating nicely formatted tables in knitted reports |
| **FSA** | Provides `dunnTest()` for post-hoc pairwise comparisons |

The following chunk checks whether these packages are installed, installs them if necessary, and then loads them.

```{r import packages}
pkg_needed <- c(
  "tidyverse",   # includes ggplot2, dplyr, readr, etc.
  "dplyr",       # explicit for readability
  "ggplot2",     # explicit for readability
  "janitor",
  "lubridate",
  "kableExtra",
  "FSA",   # for dunnTest
  "stringr"
  
)

to_install <- setdiff(pkg_needed, rownames(installed.packages()))
if (length(to_install) > 0) {
  try(install.packages(to_install, dependencies = TRUE, repos = "https://cloud.r-project.org"), silent = TRUE)
}

invisible(lapply(pkg_needed, function(p) {
  suppressPackageStartupMessages(library(p, character.only = TRUE))
}))
```

## 1.3 Import the dataset

In this sub chapter we pipe-in the dataset.

This dataset contains LLM forecasts, Polymarket prices, market metadata, and market resolutions.

```{r Import data}
data_raw <- readr::read_csv("master_forecasting_With_vol_edited.csv")

#Clean column names (e.g., 'Market ID' -> 'market_id')

data <- data_raw %>%
janitor::clean_names()
```

## 1.4 Standardize the date column

The date column is converted to the Date class to support temporal analysis later in the report.

```{r standardize date}

# Convert "date" column from DD-MM-YYYY to Date class
if ("date" %in% names(data)) {
  data <- data %>%
    mutate(date = lubridate::dmy(date))
}

# If end_date is also a string in DD-MM-YYYY format, convert it too
if ("end_date" %in% names(data)) {
  data <- data %>%
    mutate(end_date = lubridate::dmy(end_date))
}

```

### 1.5.1 Inspect the structure of the dataset

To see how the dataset is looking a quick structural check was performed, to verify that columns were imported with correct data types and that the dataset aligns with expectations.

```{r inspect the data sturcture}
glimpse(data)


```

**Interpretation**

The dataset is made up of 15,276 forecast observations across multiple Polymarket prediction markets.
At this stage the dataset contains raw LLM forecast probabilities, and their corresponding market prices (**yes_price** and **no_price**), prompt type labels (**Zero-shot** and **CoT**), model identifiers and market-level metadata (including **end_date** and **volume_num**).
As expected, **volume_num** and **liquidity_num** contain missing values before August 8, 2025, that is due to a malfunction in the data collection process that was caught and fixed on that day in the data collection phase this research.

### 1.5.2 Clean up column structure

Through inspecting the structure of the data set 4 redundant columns were identified that only include NA values, as they serve no purpose they are removed.

```{r column cleanup}
data <- data %>%
  select(-x20:-x23)

```

## 1.6 Inspect missingness

This optional diagnostic shows which variables have the most missing values.
We expect to see that end_date, volume_num, and liquidity_num are mostly NA before 2025-08-08.

```{r inspect missingness }
#Calculate share of missing values per column

na_summary <- data %>%
summarise(across(everything(), ~ mean(is.na(.x)))) %>%
tidyr::pivot_longer(everything(), names_to = "column", values_to = "na_share") %>%
arrange(desc(na_share))

#Display top 15 columns with most missing values

head(na_summary, 15)
```

**Interpretation**

The missingness diagnostic indicates that **end_date**, **volume_num**, **days_to_end**, **liquidity_num** and **research_tokens** all have a high missingness share.
This is due to the previously mentioned issue in the data collection phase and was to be expected.
Their missingness is therefore systematic and expected, not a data quality concern.

All other key forecasting variables show minimal missingess, indicating that primary inputs required for comparing LLM and Polymarket forecasts are complete.

## 1.7 Chapter summary

In this chapter, a reproducible analysis environment was established along with verifying that all software dependencies are available.
The merged dataset of LLM and Polymarket predictions was imported, and column names were standardized to ensure a consistent format during the analysis.
The data fields were converted to a conventional Date object to enable temporal segmentation later in the analysis.

A structural inspection confirmed that the dataset contains all the expected forcasting variables.

# Chapter 2 Data exploration

In this chapter we start exploring the data set after it has been imported and cleaned.
We aim to understand what each row represents, how many markets, models, and prompt types are present, and to verify that our date and numeric fields look correct.
With the goal of giving us confidence that the dataset structure aligns with what we expect before moving into analysis.

## 2.0 Prompt correction

```{r prompt correction}
# Standardize prompt_type labels
data <- data %>%
  mutate(
    prompt_type = dplyr::recode(prompt_type,
                                "CoT" = "Chain-of-Thought",
                                "Zero-shot" = "Zero-shot"),
    prompt_type = factor(prompt_type, levels = c("Zero-shot", "Chain-of-Thought"))
  )
```

## 2.1 Overview of dataset size and scope

The following chunk counts how many total rows, unique markets, models, and prompt types exist in the dataset.
This tells us whether our data collection captured the expected breadth of information.

```{r Scope and size of data}
#Summarize how many rows (forecasts) exist in total and how many unique markets, models, and prompt types are represented.

data %>%
summarise(
total_rows = n(), # total number of forecast entries
unique_markets = n_distinct(id), # number of unique Polymarket markets
unique_models = n_distinct(model), # number of LLMs used
unique_prompts = n_distinct(prompt_type), # number of prompt types (Zero-shot, CoT, etc.)
first_collection = min(date, na.rm = TRUE), # first day of data collection
last_collection = max(date, na.rm = TRUE) # most recent collection date
)
```

**Interpretation**

Here we have an overview of the dataset, by looking at the overview we can see that there are still some abnormalities that have to be dealt with. We know there are only 6 models used, there are only 53 markets that were observed and only 2 prompting strategies, while the table claims there are more. This is likely due to empty rows in the dataset that will be dealt with later in the report.

## 2.2 Check for resolved vs unresolved markets

Forecast accuracy can only be evaluated on markets that have resolved.
To check if there are any unresolved markets we will create a quick table showing, how many resolved and unresolved markets there is in the dataset.

```{r resolved vs unresolved}
#Count how many rows have a non-missing 'resolution' value (0 or 1) versus those that are still open (NA).

data %>%
mutate(resolved = !is.na(resolution)) %>%
count(resolved) %>%
mutate(share = n / sum(n))
```

**Interpretation**

The table shows that there are 12 rows with unresolved markets.
A manual inspection of the dataset showed that those markets correspond to dates when no forecasts were generated to keep a consistent timeline.
We discovered that 11 market rows have NA in the resolution column.

#### 2.2.1 Handling intentionally missing resolution values

To make sure that the empty rows corresponding to the missing observations days don't interfere with later analysis a dummy variable was created, **is_placeholder**.
This ensures that they can be safely excluded form any later accuracy or statistical analysis.

```{r handling missing res val}
#Identify placeholder rows:
#These are rows with NA resolution AND no LLM forecast or market price.
#They exist only to fill the timeline for certain dates.

data <- data %>%
mutate(
# Logical flag for placeholder rows
is_placeholder = if_else(
is.na(resolution) &
(is.na(forecast) | forecast == "") &
(is.na(yes_price) | yes_price == ""),
TRUE, FALSE
)
)

#Count how many placeholders were detected

data %>%
count(is_placeholder)


```

**Interpretation**

12 placeholder rows contain no meaningful data.
They have been flagged using \*\*is\_\_placeholder\*\* **=** **TRUE** and excluded from all later accuracy analysis

## 2.3 Sanity-check key numeric fields

This chunk inspects ranges of forecast (LLM probability) and yes_price (Polymarket implied probability), ignoring placeholder rows.
It reports mins/maxes so we can catch parsing issues (e.g., character strings) or out-of-range values.

```{r snaity check on key numeric findings}

#Work on non-placeholder rows only (placeholders have no analytical content)

data_valid <- data %>% filter(is_placeholder == FALSE | is.na(is_placeholder))

#Peek at the current classes of our key columns

sapply(data_valid[, c("forecast", "yes_price")], class)

#Compute simple range summaries for the two key probability columns

range_summary <- data_valid %>%
summarise(
forecast_min = suppressWarnings(min(as.numeric(forecast), na.rm = TRUE)),
forecast_max = suppressWarnings(max(as.numeric(forecast), na.rm = TRUE)),
yes_min = suppressWarnings(min(as.numeric(yes_price), na.rm = TRUE)),
yes_max = suppressWarnings(max(as.numeric(yes_price), na.rm = TRUE))
)

range_summary

```

**Interpretation**  
All the LLM forecast probabilities fall within the valid probability range [0,1].
This confirms that these fields contain valid inputs that are suitable for later analysis.
No out-of-range or malformed values are present.

## 2.4 Flag and summarize market-level metadata availability

From your data-collection design, we know that end_date, volume_num, and liquidity_num are intentionally NA prior to 2025-08-08.
In this step, we explicitly create a flag to track that boundary so that any analysis depending on market-level variables can filter accordingly.

```{r market level maetdata availability}
#Define the cutoff date when market-level variables started being collected

cutoff_date <- as.Date("2025-08-08")

#Add a Boolean column that marks rows recorded on or after the cutoff

data <- data %>%
mutate(
has_market_meta = if_else(date >= cutoff_date, TRUE, FALSE)
)

#Quick summary: how many rows before and after the cutoff

data %>%
count(has_market_meta) %>%
mutate(share = round(100 * n / sum(n), 1))


```

**Interpretation**\
Approximately 67.6% of forecast rows were recorded on or after the point at which market-level metadata (end_date, volume_num, liquidity_num) became available.
The remaining rows that don't contain any market-level metadata come from the time period before market-level metadata started.

### 2.4.1 Verification of Market-Metadata Availability by Time Period

To double-check which variables were introduced after 2025-08-08, we can compute the share of missing values before and after that date.
This helps confirm that the missingness pattern matches expectations.

```{r Marketing meta diag}
meta_vars <- c("end_date", "volume_num", "liquidity_num")

#Summarize NA share before vs after cutoff

meta_na_summary <- data %>%
mutate(period = if_else(has_market_meta, "After 2025-08-08", "Before 2025-08-08")) %>%
summarise(across(all_of(meta_vars), ~ mean(is.na(.x)), .names = "{.col}_na_share"), .by = period)

#Display nicely formatted table

meta_na_summary %>%
kableExtra::kbl(
caption = "Share of missing values in market-level variables before vs after 2025-08-08",
digits = 2
) %>%
kableExtra::kable_styling(full_width = FALSE)
```

**Interpretation**

The table confirms the expected data collection pattern.
Prior to 2025-08-08, `end_date`, `volume_num`, and `liquidity_num` are entirely missing, reflecting the period before market metadata was included in the data collection process.
After this date, missingness in these fields drops to much more normal 8%, which is consistent with occasional incomplete market updates rather than structural data issues.

### 2.4.2 Handle rows with missing date

Here we: (a) inspect rows where date is missing, (b) decide how to handle them, and (c) re-compute the has_market_meta flag after the fix.
Because timeline analyses require a valid date, the safest default is to drop rows with missing date, but we’ll first review them and save a small audit table.

```{r scanning for missing data}

#Inspect rows where 'date' is NA (these produced has_market_meta == NA)

missing_date_rows <- data %>%
filter(is.na(date)) %>%
select(id, question, model, prompt_type, forecast, yes_price, resolution)

#Show how many and preview a few (if any)

tibble(
n_missing_date = nrow(missing_date_rows)
)

head(missing_date_rows, 10)

```

After inspection it is clear that the two rows with missing date are completely empty and will be removed in the following chunk

```{r removing empty rows}
data <- data %>%
filter(!is.na(date))

#Confirm removal

tibble(
remaining_rows = nrow(data),
remaining_missing_date = sum(is.na(data$date))
)
```

## 2.5 Chapter Summary

In this chapter, we confirmed that the dataset contains only valid forecast observations.
Furthermore we verified that all markets in the dataset have a valid resolution, that enables reliable accuracy evaluations later on.
Placeholders and empty rows that don't contribute any meaningful data to the analysis were removed as well.
Finally, we created a new variable `has_market_meta` to distinguish between periods before and after market-level metadata collection began.

# Chapter 3 Data preperation


### 3.1 Filter to resolved markets

Before we can evaluate forecast accuracy, we need to ensure that each row in the analysis corresponds to a resolved market, that is, a market where the outcome is already known.
In Polymarket data, resolution values are typically binary:

1 → event occurred (YES)

0 → event did not occur (NO)


```{r filter resolved markets}
#Keep only rows where resolution is known (0 or 1)
#Exclude placeholders explicitly to avoid including empty timeline rows.

data_resolved <- data %>%
filter(
!is.na(resolution), # must have an outcome
is_placeholder == FALSE # remove artificial timeline rows
)

#Summarize to confirm the number of usable observations

data_resolved %>%
summarise(
n_total = n(),
n_markets = n_distinct(id),
n_models = n_distinct(model),
n_prompts = n_distinct(prompt_type)
)

```

**Interpretation**

After filtering out unresolved markets and placeholder rows, the dataset consists of 15,264 forecast observations across 53 markets, generated by 6 different language models using 2 prompting strategies.
All remaining rows correspond to forecasts with known outcomes, making the dataset suitable for computing accuracy metrics.

## 3.2 Compute Brier Scores

The **Brier Score** measures the accuracy of probabilistic predictions.\
It is defined as:

$$
\text{Brier Score} = (p - o)^2
$$

where:

-   *p* = predicted probability of the event occurring, and\
-   *o* = actual outcome (1 if event occurred, 0 if not).

A lower Brier score indicates better performance.\
A perfect forecast has a score of 0, and the worst possible forecast (certain but wrong) has a score of 1.

In this section we’ll:

(a)  Calculate Brier scores for both **LLM forecasts** (`forecast`) and **Polymarket probabilities** (`yes_price`).
(b)  Store them as new columns (`brier_llm` and `brier_market`).
(c)  Verify that these values make sense (range between 0 and 1).

```{r brier score}

data_brier <- data_resolved %>%
mutate(
# Ensure numeric type before computing
forecast = as.numeric(forecast),
yes_price = as.numeric(yes_price),

# Compute the Brier scores
brier_llm = ( forecast - resolution )^2, 
brier_market = (yes_price -  resolution )^2 

)

#Quick descriptive summary

data_brier %>%
summarise(
mean_brier_llm = mean(brier_llm, na.rm = TRUE),
mean_brier_market = mean(brier_market, na.rm = TRUE),
min_brier_llm = min(brier_llm, na.rm = TRUE),
max_brier_llm = max(brier_llm, na.rm = TRUE),
min_brier_market = min(brier_market, na.rm = TRUE),
max_brier_market = max(brier_market, na.rm = TRUE)
)

```

**Interpretation**

The Brier score has now been calculated for both LLM forecasts and Polymarket prices, the table shows that they are correctly bound between 0 and 1.
Furthermore the table shows that the Brier score for Polymarket (0.033) is lower than that of the LLM forecast (0.136), this indicates that on average Polymarket predictions are closer to the true outcomes of the markets observed.
This gives a strong indication that the market (Polymarket) outperforms the LLMs probabilistic accuracy, although more detailed comparisons and analysis will follow in later sections of this report.

### 3.2.1 Diagnose missing Brier components

This ensures that any missing or infinite values are identified before plotting or statistical testing.

```{r breir check}
#Check how many Brier values are NA or non-finite

diag_tbl <- data_brier %>%
summarise(
n_rows = n(),
n_na_forecast = sum(is.na(forecast)),
n_na_yes_price = sum(is.na(yes_price)),
n_na_brier_llm = sum(!is.finite(brier_llm)),
n_na_brier_market = sum(!is.finite(brier_market))
)

diag_tbl
```

**Interpretation**

There are 1,165 rows where the LLM forecast is missing, which results in the same number of missing `brier_llm` values.
In contrast, Polymarket probabilities (`yes_price`) are complete, and all `brier_market` scores are well-defined.

The missing LLM forecasts reflect differences in how consistently the various models produced probability forecasts across markets.
In later analysis, we account for this imbalance to ensure that comparisons between LLM and market accuracy are based on a matched set of observations.

### 3.2.2 Visualize distributions

We can quickly summarize the average difference between LLM and Polymarket Brier scores to see which tends to be closer to the truth.

```{r brier visulizer}

#Drop non-finite values before plotting to avoid warnings 
data_brier_long <- data_brier %>% 
  select(model, prompt_type, brier_llm, brier_market) %>% 
  tidyr::pivot_longer( 
    cols = starts_with("brier_"), 
    names_to = "source", 
    values_to = "brier_score" 
    ) %>% 
  filter(is.finite(brier_score))

ggplot(data_brier_long, aes(x = brier_score, fill = source)) +
  geom_histogram(bins = 30, alpha = 0.55, position = "identity", color = tud_black, linewidth = 0.2) +
  scale_fill_manual(
    values = c(
      "brier_llm" = "#0076C2",      # col
      "brier_market" = "#E03C31"    # col
    ),
    labels = c("brier_llm" = "LLM", "brier_market" = "Polymarket")
  ) +
  labs(
    title = "Distribution of Brier Scores",
    subtitle = "Lower values indicate better forecast accuracy",
    x = "Brier Score",
    y = "Count",
    fill = "Forecast Source"
  ) +
  theme_tud()



```

**Interpretation**  
The distribution clearly shows the performance difference between Polymarket and the LLM forecasts.
On one hand Polymarket Brier scores are mostly concentrated near zero, giving an indication of rather consistently accurate probability assessment across markets.
On the other hand, LLM Brier scores are more widely spread and shifted towards higher errors, indicating higher variability and overall lower forecast accuracy.

### 3.2.3 Create complete paired dataset

This step filters out observations where forecasts are missing, keeping only observations where both Brier scores are available.
Making sure that all accuracy comparisons are made on directly matched forecasts.

```{r Paired dataset}


data_brier_complete <- data_brier %>%
filter(is.finite(brier_llm), is.finite(brier_market))

#Check how many valid rows remain

data_brier_complete %>%
summarise(
n_rows = n(),
n_markets = n_distinct(id),
n_models = n_distinct(model),
n_prompts = n_distinct(prompt_type)
)
```

**Interpretation**

After restricting the dataset to only rows where both LLM and Polymarket Brier scores are available, 14,099 paired forecasts remain.
This paired structure is required for all future accuracy comparisons, due to the fact that each statistical test evaluates whether the LLM and Polymarket differ when predicting the *same* event under the same conditions.

The number of unique markets (53), models (6), and prompt types (2) is unchanged, meaning that the filtering only removed data entries where the LLM did not generate a valid probability prediction.

### 3.2.4 Compare mean Brier scores

We now compare the average Brier scores of LLMs and Polymarket across all paired forecasts.

```{r compare mean brier scores}


data_brier_complete %>%
summarise(
mean_diff = mean(brier_llm - brier_market, na.rm = TRUE), # < 0 => LLM better
better_source = if_else(mean_diff < 0, "LLM", "Polymarket")
)
```

**Interpretation**

The mean brier score difference is positive (0.10), this indicates that Polymarket forecasts are, on average, more accurate than the LLM forecasts.

## Chapter 3.3 Validation and Exploration of Brier Scores

This section provides a deeper descriptive overview of Brier scores, comparing LLM forecasts and Polymarket probabilities across different models and prompting strategies.
We focus on distributional checks, aggregate summaries, and visual comparisons, all of which prepare us for the statistical testing in Chapter 4.

#### 3.3.1 Summarize Brier scores by model and prompt type

We start by summarizing the mean, median, and variability (standard deviation) of Brier scores for both LLMs and Polymarket, grouped by model and prompt_type.

```{r Brier score summary table}

#Summarize by model and prompt type

brier_summary <- data_brier_complete %>%
group_by(model, prompt_type) %>%
summarise(
n_obs = n(),
mean_brier_llm = mean(brier_llm, na.rm = TRUE),
sd_brier_llm = sd(brier_llm, na.rm = TRUE),
mean_brier_market = mean(brier_market, na.rm = TRUE),
sd_brier_market = sd(brier_market, na.rm = TRUE),
mean_diff = mean(brier_llm - brier_market, na.rm = TRUE),
median_diff = median(brier_llm - brier_market, na.rm = TRUE),
.groups = "drop"
) %>%
arrange(desc(mean_diff))

#Display neatly formatted summary

brier_summary %>%
kableExtra::kbl(
caption = "Brier score summary by model and prompt type",
digits = 3
) %>%
kableExtra::kable_styling(full_width = FALSE)
```

```{r brier-summary-tudelft, message=FALSE, warning=FALSE}

library(dplyr)
library(kableExtra)
library(scales)

TOTAL_EXPECTED <- 1272

brier_table <- data_brier_complete %>%
  group_by(model, prompt_type) %>%
  summarise(
    n_obs = n(),
    mean_brier = mean(brier_llm, na.rm = TRUE),

    .groups = "drop"
  ) %>%
  mutate(
    failure_rate = 1 - (n_obs / TOTAL_EXPECTED),
    failure_rate = pmin(pmax(failure_rate, 0), 1),
    failure_rate_pct = percent(failure_rate, accuracy = 0.1)
  ) %>%
  arrange(desc(n_obs), desc(mean_brier)) %>%
  select(model, prompt_type, n_obs, mean_brier, failure_rate_pct)

brier_table %>%
  kbl(
    caption = "Model Overview",
    col.names = c("Model", "Prompt type", "N", "Mean Brier score", "Failure rate"),
    digits = c(NA, NA, 0, 3, NA),
    booktabs = TRUE,
    align = c("l", "l", "r", "r", "r")
  ) %>%
  kable_styling(
    full_width = FALSE,
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = TRUE) %>%
  footnote(
    general = paste0(
      "Rows are ordered by descending N. Lower Brier scores indicate better probabilistic accuracy."
    ),
    general_title = ""
  )
```


**Interpretation**

Table 3.1 reports descriptive Brier score statistics by model and prompting strategy.
Each row represents a unique combination of model and prompt type.
The columns summarize both the level and variability of forecast accuracy for the LLMs and Polymarket baseline.

| Column | Meaning |
|--------------------------------|----------------------------------------|
| `n_obs` | Number of paired forecasts available for this model + prompt type |
| `mean_brier_llm` | Average Brier score of the LLM (lower is better) |
| `sd_brier_llm` | Variability of the LLM’s forecast accuracy |
| `mean_brier_market` | Average Brier score of Polymarket for the same forecasts |
| `sd_brier_market` | Variability of Polymarket forecast accuracy |
| `mean_diff` | Difference in mean accuracy (`brier_llm - brier_market`), where positive values indicate Polymarket is more accurate |

Polymarket consistently achieve lower Brier scores than the LLMs across the board, confirming higher average forecasting accuracy.
However there are differences between LLMs and between prompting strategies.
Models such as **DeepSeek-R1** and **Gemma-3 12B** show lower mean Brier scores compared to the Open AI GPT models.
CoT prompting generally provides a lower Brier score compared to ZS prompting, indicating that CoT prompting might increase the accuracy of LLM predictions.

#### 3.3.2 Visual comparison: boxplots of Brier scores by model

Now let’s visualize the distributions of Brier scores per model, split by forecast source (LLM vs Polymarket).

```{r boxplots by model}

data_brier_long <- data_brier_complete %>%
  select(model, prompt_type, brier_llm, brier_market) %>%
  tidyr::pivot_longer(
    cols = starts_with("brier_"),
    names_to = "source",
    values_to = "brier_score"
  ) %>%
  filter(is.finite(brier_score))

ggplot(data_brier_long, aes(x = model, y = brier_score, fill = source)) +
  geom_boxplot(outlier.shape = NA, alpha = 0.75) +
  coord_flip() +
  scale_fill_manual(
    values = c(
      "brier_llm" = "#6F1D77",
      "brier_market" = "#0076C2"
    ),
    labels = c("brier_llm" = "LLM", "brier_market" = "Polymarket")
  ) +
  labs(
    title = "Distribution of Brier Scores by Model",
    subtitle = "Comparing LLM forecast accuracy vs Polymarket",
    x = "Model",
    y = "Brier Score",
    fill = "Forecast Source"
  ) +
  theme_tud()

```

**Interpretation**

LLM performance varies by model.
with DeepSeek and Gemma-3 showing lower error compared to the Open AI models, especially DeepSeek-R1.
Polymarket forecasts are consistently more accurate, with Brier scores concentrated near zero.

### 3.3.3 Visual comparison: boxplots by prompt type

To explore weather prompting strategies influence performance the following chart was created.

```{r boxplot promt type}
tud_colors <- c(
"Zero-shot" = "#6F1D77", # Light purple
"Chain-of-Thought" = "#0076C2" # Royal blue
)

ggplot(
  brier_summary,
  aes(
    x = reorder(model, median_diff),
    y = median_diff,
    color = prompt_type
  )
) +
  geom_point(size = 3, stroke = 1.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  coord_flip() +
  scale_color_manual(values = tud_colors) +
  labs(
    title = "Median LLM – Polymarket Brier Score Difference",
    subtitle = "Values above 0 = Polymarket more accurate | Values below 0 = LLM more accurate",
    x = "Model",
    y = "Median Difference in Brier Score",
    color = "Prompt Type"
  ) +
  theme_tud() +
  theme(
    legend.position = "right",
    legend.title = element_text(face = "bold", size = 11),
    legend.text = element_text(size = 10),
    plot.title = element_text(face = "bold", size = 13),
    plot.subtitle = element_text(size = 11),
    axis.title = element_text(size = 11),
    axis.text = element_text(size = 10)
  )
```

**Interpretation**

The results are consistent with previous findings, that **Polymarket consistently achieves lower Brier scores** compared to the LLM forecasts, that is shown by the positive mean difference across all models.
The effect of prompting strategies is rather small, *Chain-of-Thought* slightly improves performance for some models, particularly for *DeepSeek-R1* and for *Chat GPT 3.5*, although overall ranking between models remains mostly stable under both prompting approaches.

## Chapter 3.4 Data Validation and Preparation for Testing

Lets take a moment before proceeding with the statistical analysis, in this section we will check if the dataset is clean, consistent and nicely structured for comparisons.\
We also confirm that all **Brier score values** are valid, no unresolved markets remain, and that each observation is correctly paired between **LLM** and **Polymarket** forecasts based on market, date, and model.\
These checks ensure that the dataset is ready for statistical testing later on.

### 3.4.1 Check for missing or infinite values

Confirming that all Brier score columns contain finite, non-missing values.

```{r check for NA or inf brier}
#inspect missing or invalid entries in Brier score columns

brier_data_check <- data_brier_complete %>%
summarise(
missing_llm = sum(is.na(brier_llm)),
missing_market = sum(is.na(brier_market)),
infinite_llm = sum(!is.finite(brier_llm)),
infinite_market = sum(!is.finite(brier_market))
)

brier_data_check %>%
kableExtra::kbl(
caption = "Check for missing or infinite Brier score values",
digits = 0
) %>%
kableExtra::kable_styling(full_width = FALSE)
```

**Interpretation**

From the results of 3.4.1 there are no fields with infinite or missing brier scores.

#### 3.4.2a Data Preservation Step

Since there were no missing or infinite values, no cleaning was required.
However, to ensure consistency and reproducibility across the rest of the workflow, we create a preserved dataset called `data_brier_valid`.\
This dataset serves as the working copy for all further analysis and statistical testing, ensuring results remain consistent and stable.

```{r data preservation}
#Create a preserved working dataset for Brier score analysis.
#This ensures that later chunks (correlation, testing, etc.) always reference a consistent data frame, even if no cleaning was needed.

data_brier_valid <- data_brier_complete

#Quick sanity check

tibble::tibble(
n_rows = nrow(data_brier_valid),
any_non_finite = sum(!is.finite(data_brier_valid$brier_llm) |
!is.finite(data_brier_valid$brier_market))
)
```

**Interpretation**

The dataset `data_brier_valid` contains 14,099 valid observations, with no non-finite or missing Brier score values.\
This confirms that the dataset is complete and valid, making it ready for further statistical analysis.

### 3.4.3 Pairwise correlation check

Before continuing any further, a Pearson correlation check was performed to confirm that LLM forecasts and Polymarket probabilities are correlated.
Two tests were performed, one based on the **Brier scores** and another on the **raw forecast probabilities**.

```{r correlation check}
### 3.4.3 Pairwise correlation check


# --- 1. Correlation of Brier Scores ---
cor_test_brier <- cor.test(
  data_brier_valid$brier_llm,
  data_brier_valid$brier_market,
  method = "pearson"
)

# --- 2. Correlation of Raw Forecast Probabilities ---
cor_test_forecast <- cor.test(
  data_brier_valid$forecast,
  data_brier_valid$yes_price,
  method = "pearson"
)

# --- Combine into summary table ---
cor_tibble <- tibble(
  correlation_type = c("Brier Score (LLM vs Polymarket)", "Raw Forecast (LLM vs Polymarket)"),
  r = c(round(cor_test_brier$estimate, 3), round(cor_test_forecast$estimate, 3)),
  p_value = c(signif(cor_test_brier$p.value, 3), signif(cor_test_forecast$p.value, 3))
)

cor_tibble %>%
  kableExtra::kbl(
    caption = "Pearson correlation between LLM and Polymarket forecasts (Brier scores and raw probabilities)"
  ) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

**Interpretation**

The Pearson correlation between LLM and Polymarket Brier scores is **r = 0.294** with a **p-value \< 0.001**.
This indicates a modest positive correlation between them with a statistical significance.

The **correlation between raw forecast probabilities** is considerably stronger **r = 0.576**, with a **p-value \< 0.001**.
This indicates that more often than not LLMs and Polymarket assign a similar probability to the same events.

Looking at the results together, it shows that the two forecasting systems are likely exposed to similar information signals, although they differ in how accurately they are able to assign probabilities to them.

### 3.4.4 Summary statistics before testing

To end this sub-chapter with a compact overview of the cleaned dataset, summarizing the distribution of Brier scores across all models.

```{r summary of clean data}

brier_summary_final <- data_brier_valid %>%
  summarise(
    mean_llm = mean(brier_llm),
    sd_llm = sd(brier_llm),
    mean_market = mean(brier_market),
    sd_market = sd(brier_market),
    mean_diff = mean(brier_llm - brier_market)
  )

brier_summary_final %>%
  kableExtra::kbl(
    caption = "Summary statistics of cleaned Brier score data",
    digits = 3
  ) %>%
  kableExtra::kable_styling(full_width = FALSE)

```

**Interpretation**

The cleaned dataset `data_brier_valid` shows no missing or invalid values, confirming that all Brier score pairs are matched up correctly, valid and ready for further analysis.\
Looking across all observations, the **mean LLM Brier score is 0.136** with a standard deviation of **0.157**, while the **mean Polymarket Brier score is 0.034** with a standard deviation of **0.123**.\
The **average difference of 0.103** indicates that on average, Polymarket forecasts are more accurate than LLM forecasts, that is consistent with our previous findings.

## Chapter 3.5 Forecast Completeness Check

This section evaluates the **completeness and consistency**.\
Before conducting formal hypothesis tests, it is important to verify the completeness of the forecast data across models and prompting strategies.
This step identifies any missing forecasts, determines which models have missing forecasts, and whether prompt type had an impact on data collection consistency.

### 3.5.1 Count missing forecasts

We start by checking how many forecasts are missing overall, and then break them down by model and prompt type.

```{r missing forcast check}
#Exclude rows without model or prompt type (NA values)
#Then calculate missing forecast share by model and prompt type

missing_summary <- data %>%
filter(!is.na(model), !is.na(prompt_type)) %>% # <-- filter added
group_by(model, prompt_type) %>%
summarise(
total_obs = n(),
missing_forecast = sum(is.na(forecast)),
missing_share = round(100 * mean(is.na(forecast)), 2),
.groups = "drop"
) %>%
arrange(desc(missing_share))

#Display nicely formatted table

missing_summary %>%
kableExtra::kbl(
caption = "Share of missing forecasts by model and prompt type (excluding NA rows)",
digits = 2
) %>%
kableExtra::kable_styling(full_width = FALSE)
```


**Interpretation:**

The completeness check shows a big variation in forecast consistency across models and prompting strategies.

*DeepSeek* models show the highest missing shares, with *deepseek_deepseek_chat_v3_0324_free* missing around **36–38%** of its forecasts and *deepseek_r1_free* missing around **6–7%**.

*OpenAI* models (*gpt_3_5_turbo_0613* and *gpt_4o_mini*) achieved **full coverage** with no missing forecasts.
*Gemma* models maintained a very respectable forecast consistency with less than **2%** missingness.

### 3.5.2 Visualizing Forecast Missingness

```{r missingness by model}
miss_by_model <- data %>%
dplyr::filter(!is.na(model)) %>%
dplyr::summarise(
total_obs = dplyr::n(),
missing_forecast = sum(is.na(forecast)),
missing_share = round(100 * mean(is.na(forecast)), 2),
.by = model
) %>%
dplyr::arrange(dplyr::desc(missing_share))

ggplot(miss_by_model, aes(x = model, y = missing_share)) +
geom_col(fill = tud_sec["Royal blue"]) + # fixed TU Delft blue
geom_text(
aes(label = paste0(missing_share, "%")),
vjust = -0.35, size = 3, color = tud_black
) +
labs(
title = "Share of Missing Forecasts by Model",
subtitle = "Aggregated across prompt types",
x = "Model",
y = "Missing forecasts (%)"
) +
theme_tud() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
expand_limits(y = max(miss_by_model$missing_share, na.rm = TRUE) * 1.15)
```

**Interpretation:**\
This plot confirms the earlier completeness results.\
*DeepSeek* models have the highest percentage of missing forecasts, while *OpenAI* and *Gemma* models remain nearly complete.

#### 3.5.2B Missing forecasts by prompt type
Lets look at if missing forecasts are effected by prompt type.

```{r missingness by prompt}

# Canonical labels for consistency across the thesis
data_std <- data %>%
  dplyr::mutate(
    prompt_type = dplyr::recode(
      prompt_type,
      "CoT" = "Chain-of-Thought",
      "COT" = "Chain-of-Thought",
      "cot" = "Chain-of-Thought",
      "ZS"  = "Zero-shot",
      "zero-shot" = "Zero-shot"
    )
  )

miss_by_prompt <- data_std %>%
  dplyr::filter(!is.na(prompt_type)) %>%
  dplyr::summarise(
    total_obs = dplyr::n(),
    missing_forecast = sum(is.na(forecast)),
    missing_share = round(100 * mean(is.na(forecast)), 2),
    .by = prompt_type
  ) %>%
  dplyr::mutate(
    prompt_type = factor(prompt_type, levels = c("Zero-shot","Chain-of-Thought"))
  ) %>%
  dplyr::arrange(dplyr::desc(missing_share))

ggplot(miss_by_prompt, aes(x = prompt_type, y = missing_share, fill = prompt_type)) +
  geom_col() +
  geom_text(aes(label = paste0(missing_share, "%")),
            vjust = -0.35, size = 3, color = tud_black) +
  labs(
    title = "Share of Missing Forecasts by Prompt Type",
    subtitle = "Aggregated across models",
    x = "Prompt type",
    y = "Missing forecasts (%)",
    fill = "Prompt type"
  ) +
  scale_fill_manual(
    values = c(
      "Zero-shot" = "#6CC24A",          # Green
      "Chain-of-Thought" = "#A50034"    # Burgundy
    ),
    drop = FALSE
  ) +
  theme_tud() +
  expand_limits(y = max(miss_by_prompt$missing_share, na.rm = TRUE) * 1.15)
```

**Interpretation:**

Missingness is similar across prompt types, with little to no difference between Zero-shot and Chain-of-Thought once model effects are pooled.

### 3.5.3 Combined completeness overview

This final figure in this sub-chapter displays the missingness analysis across both **models** and **prompt types**.\
Each model is represented by two bars, *Zero-shot* and *Chain-of-Thought*, showing the percentage of missing forecasts.\
Models are ordered by missingness.

```{r Chapter 3 final figure}
library(stringr)

# 1) Prepare data: drop true NAs, recode variants to canonical labels, enforce order
miss_final <- data %>%
  dplyr::filter(!is.na(model), !is.na(prompt_type)) %>%
  dplyr::mutate(
    prompt_type = str_squish(prompt_type),
    prompt_type = case_when(
      str_to_lower(prompt_type) %in% c("zero-shot", "zero shot", "zs") ~ "Zero-shot",
      str_to_lower(prompt_type) %in% c("cot", "co-t", "chain-of-thought", "chain of thought", "chain_of_thought") ~ "CoT",
      TRUE ~ prompt_type
    ),
    # factor AFTER recode; anything still not matched stays as-is (for debugging)
    prompt_type = factor(prompt_type, levels = c("Zero-shot", "CoT"))
  ) %>%
  dplyr::summarise(
    total_obs = dplyr::n(),
    missing_forecast = sum(is.na(forecast)),
    missing_share = 100 * mean(is.na(forecast)),
    .by = c(model, prompt_type)
  )

# Optional sanity check: should be 0
miss_final %>%
  dplyr::summarise(n_na_prompt = sum(is.na(prompt_type))) %>%
  print()

# 2) Order models by overall missingness
model_order <- miss_final %>%
  dplyr::summarise(overall_missing = mean(missing_share), .by = model) %>%
  dplyr::arrange(dplyr::desc(overall_missing)) %>%
  dplyr::pull(model)

miss_final <- miss_final %>%
  dplyr::mutate(model = factor(model, levels = model_order))

# 3) Plot
ggplot(miss_final, aes(x = model, y = missing_share, fill = prompt_type)) +
  geom_col(position = position_dodge(width = 0.75), width = 0.7) +
  geom_text(
    aes(label = paste0(round(missing_share, 1), "%")),
    position = position_dodge(width = 0.75),
    vjust = -0.35, size = 3, color = tud_black
  ) +
  labs(
    title = "Share of Missing Forecasts by Model and Prompt Type",
    subtitle = "Two columns per model: Zero-shot and CoT",
    x = "Model",
    y = "Missing forecasts (%)",
    fill = "Prompt type"
  ) +
  scale_fill_manual(values = c("Zero-shot" = "#6CC24A", "CoT" = "#A50034"), drop = FALSE) +
  theme_tud() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  expand_limits(y = max(miss_final$missing_share, na.rm = TRUE) * 1.15)
```

**Interpretation:**

The figure in 3.5.3 shows **Zero-shot** and **CoT** bars for each model.
The missingness pattern remains the same, with DeepSeek having the highest one, and with OpenAI and Gemma nearly complete, having few to non missing observations.

## 3.6 Temporal Accuracy Comparison (Early vs. Mid vs. Late Forecasts)

To examine whether LLM forecasting accuracy changes over the course of a market’s lifetime, each market-model pair is evaluated at three distinct points in time: the beginning (first available observation), the middle (halfway point between the first and last observation), and the end (final observation).
This allows us to assess whether LLMs perform better at the early stages of market formation or closer to market resolution, when more information is typically available.
The resulting comparison provides insight into the temporal stability and adaptability of LLM-based forecasts relative to Polymarket probabilities.

```{r temporal accuracy comparison}


base_input <- if (exists("data_brier_valid")) data_brier_valid else data

#Helper: coerce a resolution-like column to {0,1}

coerce_resolution01 <- function(x) {
if (is.logical(x)) return(as.integer(x))
if (is.numeric(x)) return(as.numeric(x))
if (is.character(x)) {
x_up <- toupper(trimws(x))
return(dplyr::case_when(
x_up %in% c("YES","TRUE","1","Y") ~ 1,
x_up %in% c("NO","FALSE","0","N") ~ 0,
TRUE ~ NA_real_
))
}
as.numeric(x)
}

#1) Ensure brier_llm / brier_market exist (try to find or compute)

ensure_brier_cols <- function(df) {
nm <- names(df)

has_brier <- all(c("brier_llm","brier_market") %in% nm)
if (has_brier) return(df)

#Try to locate probability columns

llm_prob_candidates <- c("forecast","llm_prob","prob_llm","llm_forecast","pred_llm")
market_prob_candidates <- c("yes_price","market_prob","polymarket_prob","price_yes","pm_prob")
reso_candidates <- c("resolution","resolved","outcome","result")

llm_prob <- llm_prob_candidates[llm_prob_candidates %in% nm][1]
market_prob <- market_prob_candidates[market_prob_candidates %in% nm][1]
reso_col <- reso_candidates[reso_candidates %in% nm][1]

if (is.na(llm_prob) || is.na(market_prob) || is.na(reso_col)) {
stop("Could not find required columns to compute Brier scores.\n",
"Looked for LLM prob in: ", paste(llm_prob_candidates, collapse=", "),
"\nMarket prob in: ", paste(market_prob_candidates, collapse=", "),
"\nResolution in: ", paste(reso_candidates, collapse=", "), call. = FALSE)
}

df %>%
mutate(
.resolution01 = coerce_resolution01(.data[[reso_col]]),
brier_llm = (.data[[llm_prob]] - .resolution01)^2,
brier_market = (.data[[market_prob]] - .resolution01)^2
) %>%
select(-.resolution01)
}

#2) Build the working frame with required columns

base_df <- base_input %>%
ensure_brier_cols() %>%
mutate(date = as.Date(date)) %>%

#keep core columns, preserve extras if present

select(id, model, prompt_type, date, brier_llm, brier_market, everything())

#Quick diagnostic

cat(">> 3.6 using columns:",
paste(intersect(c("brier_llm","brier_market","forecast","yes_price","resolution"), names(base_df)),
collapse=", "), "\n")

#3) Compute target dates per (id, model, prompt_type)

temporal_points <- base_df %>%
group_by(id, model, prompt_type) %>%
summarise(
first_date = min(date, na.rm = TRUE),
last_date = max(date, na.rm = TRUE),
.groups = "drop"
) %>%
mutate(
mid_date = first_date + floor(as.numeric(difftime(last_date, first_date, units = "days"))/2)
)

#4) Long targets and nearest available observation per stage

temporal_targets <- temporal_points %>%
pivot_longer(c(first_date, mid_date, last_date),
names_to = "stage_raw", values_to = "target_date") %>%
mutate(stage = recode(stage_raw,
first_date="first", mid_date="mid", last_date="last")) %>%
select(-stage_raw)

temporal_data <- base_df %>%
inner_join(temporal_targets, by = c("id","model","prompt_type")) %>%
mutate(abs_diff_days = abs(as.numeric(difftime(date, target_date, units = "days")))) %>%
group_by(id, model, prompt_type, stage) %>%
slice_min(abs_diff_days, n = 1, with_ties = FALSE) %>%
ungroup() %>%
mutate(brier_diff = brier_llm - brier_market)

#5) Summary

temporal_summary <- temporal_data %>%
group_by(stage) %>%
summarise(
mean_diff = mean(brier_diff, na.rm = TRUE),
median_diff = median(brier_diff, na.rm = TRUE),
sd_diff = sd(brier_diff, na.rm = TRUE),
n = n(),
.groups = "drop"
) %>%
arrange(factor(stage, levels = c("first","mid","last")))

kableExtra::kbl(
temporal_summary,
digits = 4,
caption = "Mean and median Brier score differences (LLM − Polymarket) at start, middle, and end of market lifetimes"
) %>%
kableExtra::kable_styling(full_width = FALSE)
```

**Interpretation**\
The mean Brier score difference between LLM and Polymarket forecasts remains rather stable over the lifespan of the markets.\
At the **start** the mean difference is 0.107, that slightly decreases as time moves on and we reach the **midpoint**, down to 0.097.
It rises again to 0.116 at the **end**.\
These numbers suggest that the accuracy of the LLM Forecasts dose not significantly increase over the lifetime of the markets.
The slight dip in the **midpoint** suggests that there might be a sweet spot there where the LLM forecasts are closer to catching up with the more accurate Polymarket forecasts.

### 3.6.1 Visualization of Temporal Accuracy Comparison

We visualize how the Brier score difference between LLM and Polymarket evolves over the three market stages.\
The first figure displays stage-level mean differences with 95% confidence intervals, while the second figure shows the full distribution of individual market-level differences.

```{r Mean Brier difference by stage with 95% CI}
stage_pal <- c(
"first" = "#0076C2", # Royal blue
"mid" = "#6CC24A", # Green
"last" = "#A50034" # Burgundy
)

#Summary statistics with CI

stage_sum <- temporal_data %>%
dplyr::group_by(stage) %>%
dplyr::summarise(
mean_diff = mean(brier_diff, na.rm = TRUE),
se = stats::sd(brier_diff, na.rm = TRUE) / sqrt(dplyr::n()),
n = dplyr::n(),
.groups = "drop"
) %>%
dplyr::mutate(
ci_low = mean_diff - 1.96 * se,
ci_high = mean_diff + 1.96 * se,
stage = factor(stage, levels = c("first", "mid", "last"))
)

#Figure A: mean difference with 95% CI

ggplot(stage_sum, aes(x = stage, y = mean_diff, fill = stage)) +
geom_col(width = 0.65) +
geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = 0.18, size = 0.6, color = tud_black) +
geom_text(aes(label = sprintf("%.3f", mean_diff)), vjust = -0.45, size = 3, color = tud_black) +
scale_fill_manual(values = stage_pal, guide = "none") +
labs(
title = "Mean Brier Difference by Market Stage",
subtitle = "Difference = LLM − Polymarket (lower values indicate better LLM accuracy)",
x = "Stage",
y = "Mean difference (Brier)"
) +
theme_tud() +
expand_limits(y = max(stage_sum$ci_high, na.rm = TRUE) * 1.15)
```

```{r Distribution of Brier differences by stage}

temporal_data$stage <- factor(temporal_data$stage, levels = c("first","mid","last"))

ggplot(temporal_data, aes(x = stage, y = brier_diff, fill = stage)) +
geom_boxplot(width = 0.65, outlier.alpha = 0.25) +
scale_fill_manual(values = stage_pal, guide = "none") +
labs(
title = "Distribution of LLM − Polymarket Brier Differences by Stage",
subtitle = "Boxes show interquartile range; line indicates median. Values > 0 mean Polymarket is more accurate.",
x = "Stage",
y = "Brier difference"
) +
theme_tud()
```

**Interpretation**  
Across all market stages, the mean Brier difference remains small and stable.\
The boxplot shows that there is a consistent variance throughout the market lifetime, indicating that the forecasting accuracy of the LLMs dose not get systematically better over time, rather that they have a pretty consistent accuracy over the lifespan of the markets.

## Summary of Chapter 3

This chapter investigated if the forecasting data was complete, consistent, and ready for analysis.\
All Brier score values were valid, identified a positive correlation between LLM and Polymarket forecasts, and confirmed that forecasts were correctly matched.\
LLM forecasts had a worse Brier score than Polymarket forecasts on average, indicating that Polymarket forecasts are generally more accurate.\
Some models had missing observations with **DeepSeek-R1** having the highest missingenss percentage.\
Furthermore, accuracy across time stayed rather consistent, as early, mid, and late forecasts only differ minimally.

Now the dataset is clean, balanced, and ready for the statistical testing in Chapter 4.

# Chapter 4 Statistical Testing

This chapter applies statistical testing to determine whether the differences in forecast accuracy between LLM and Polymarket predictions are statistically significant.
Since the data are paired (each market has both LLM and Polymarket Brier scores) and may not follow a normal distribution, we use the Wilcoxon signed-rank test, a non-parametric alternative to the paired t-test.
We also compute effect sizes to assess the magnitude of these differences.

### 4.1 Test setup

We begin by restating the hypotheses:

-   **H₀ (null hypothesis):** There is no significant difference between LLM and Polymarket Brier scores.\
-   **H₁ (alternative hypothesis):** LLM and Polymarket Brier scores differ significantly.

The test uses paired Brier scores for the same markets, with the difference defined as:

$$
Diff_i = \text{Brier}_{LLM, i} - \text{Brier}_{Polymarket, i}
$$

Positive differences indicate better performance (lower error) for Polymarket.

### 4.1.1 Descriptive Overview of Differences

Before applying the Wilcoxon signed-rank test, lets examine the basic distribution of the Brier score differences between the LLM and Polymarket forecasts.

```{r descriptive overview of diff}
#Use the validated Brier dataset
#Difference = LLM Brier - Polymarket Brier

data_test <- data_brier_valid %>%
dplyr::mutate(diff_brier = brier_llm - brier_market)

#Quick descriptive summary of differences

summary(data_test$diff_brier)
```

**Interpretation**

The differences in Brier scores range from -0.81 - 1.00, with a median of 0.06 and a mean of 0.10.
Seeing how most values are positive, that indicates that Polymarket forecasts tend to achieve a lower Brier scores and there by more accuracy than LLMs on average.\
However the min of -0.807 shows that the LLMs forecasts did outperform the ones on the Polymarket in certain markets.

This indicates that no single forecasting source dominates across all markets.
Rather, the advantage is statistical and varies market to market.
The dashed vertical line marks the average difference (≈ 0.10), reinforcing that Polymarket forecasts are typically, but not always, more accurate.

Further more, skewed shape of the distribution supports the use of a non-parametric paired test rather than a t-test.

## 4.2 Wilcoxon signed-rank test


### 4.2.1 Wilcoxon signed-rank test Brier

We perform the Wilcoxon signed-rank test to evaluate whether the median difference is significantly different from zero.

```{r Wilcoxon signed rank test}
#Number of paired observations

n_pairs <- nrow(data_test)

#Wilcoxon signed-rank test

wilcox_result <- wilcox.test(
data_test$brier_llm,
data_test$brier_market,
paired = TRUE,
alternative = "two.sided",
conf.int = TRUE
)

#Display test results neatly, including n

result_table <- broom::tidy(wilcox_result) %>%
dplyr::mutate(n = n_pairs)

result_table
```

**Interpretation**

The Wilcoxon signed rank test results show a median diffrence of 0.078, with a tight confidence interval 0.076-0.080.\
The test has a p-value \< 0.001, showing that the difference is statistically significant.\
Seeing as how the median difference is positive, we can derive the concoction that Polymarket achieves a lower Brier score then the LLMs in the majority of the paired market comparisons.
This suggests that the gap seen in the descriptive results is not due to a chance.
Rather that it is systematic and holds across the full dataset.

### 4.2.2 Wilcoxon signed-rank test absoulute

To complement the earlier test on Brier score differences, now we compute the **absolute forecast errors** of the forecasts.\
By implementing the absolute errors, we check whether the ranking between LLMs and Polymarket changes or remains the same when we have removed the squaring effect of the Brier score, providing an additional view of accuracy and helps confirm if the side is consistently closer to the true resolution.

```{r ABS WILCOXON}
data_test <- data_test %>%
mutate(
resolution01 = ifelse(resolution %in% c(1, "YES", "True", TRUE), 1, 0),
abs_llm = abs(forecast - resolution01),
abs_market = abs(yes_price - resolution01)
)

n_abs <- sum(!is.na(data_test$abs_llm) & !is.na(data_test$abs_market))

wilcox_abs <- wilcox.test(
data_test$abs_llm,
data_test$abs_market,
paired = TRUE,
alternative = "two.sided",
conf.int = TRUE,
exact = FALSE
)

wilcox_abs_tidy <- broom::tidy(wilcox_abs) %>%
dplyr::mutate(n = n_abs)

wilcox_abs_tidy
```

**Interpretation**\
The absolute-error comparison is consistent with the previous Brier-score test.\
Across the 14,099 paired forecasts, absolute errors are significantly larger for the LLM forecasts then the Polymarket equivalents.\
The estimated median difference is about 0.23, with a tight confidence interval of 0.229-0.237.
The test has a p-value \< 0.001, showing that the difference is statistically significant.

These results combined with the previous results show that the Polymarket delivers consistently lower forecasting errors, regardless of if the accuracy is measured through squared error or absolute deviation.

### 4.2.3 Directional one-sided Wilcoxon tests

To further confirm what forecast performs better, we run a pair of one-sided paired Wilcoxon signed-rank tests.\
Looking at how the difference is defined in this study **brier_llm - brier_market**, positive values indicate higher Polymarket accuracy.\
The first test evaluates if Polymarket has lower Brier scores than the LLM forecasts and the second one tests the opposite.

```{r Wilcoxon DIRECTIONAL TESTS}

# Using the same data_test from 4.1
# diff_brier = brier_llm - brier_market

# H1: Polymarket better (expect diff > 0)
wilcox_greater <- wilcox.test(
  data_test$brier_llm,
  data_test$brier_market,
  paired = TRUE,
  alternative = "greater"  # median(x - y) > 0
)

# H1: LLM better (expect diff < 0)
wilcox_less <- wilcox.test(
  data_test$brier_llm,
  data_test$brier_market,
  paired = TRUE,
  alternative = "less"     # median(x - y) < 0
)

# Tidy display (some fields may not exist depending on test output)
dir_results <- dplyr::bind_rows(
  broom::tidy(wilcox_greater) %>% dplyr::mutate(hypothesis = "Polymarket better (greater)"),
  broom::tidy(wilcox_less)    %>% dplyr::mutate(hypothesis = "LLM better (less)")
) %>%
  dplyr::select(
    hypothesis,
    statistic,
    p.value,
    method,
    alternative
  )

kableExtra::kbl(
  dir_results,
  digits = 6,
  caption = "Directional Wilcoxon signed-rank tests for paired Brier score differences"
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

**Interpretation**  
Both the one-sided tests point towards the same conclusion as the two-sided results, that the Polymarket predictions are more accurate thatn those of the LLMs.\
On one hand the Polymarket-better test returns a p-value of 0, suggesting that Polymarket achieves lower Brier scores than the LLMs.\
The LLM-better test, on the other hand fails as it has a p-value of 1, and thus is not supported.\
Together the tests further confirm that the performance of the Polymarket forecasts is greater across the matched forecasts.

## 4.3 Effect Size Estimation

To see how large the effect of of the results presented in chapter 4.2 are, we perform a effect size estimation.\
Effect size is helpful to describe how large the performance gap is in terms that are comprehensible and easy to intemperate.

### 4.3.1 Effect Size Estimation Rosendals R

```{r rosendals R}
# Number of valid paired observations
n_pairs <- sum(complete.cases(data_test$brier_llm, data_test$brier_market))

# Convert p-values to Z-scores using qnorm
# Note: for p=0, use a very small epsilon to avoid Inf
p_two_sided <- max(2.2e-16, wilcox_result$p.value)
z_two_sided <- qnorm(p_two_sided / 2, lower.tail = FALSE)

p_greater <- max(2.2e-16, wilcox_greater$p.value)
z_greater <- qnorm(p_greater, lower.tail = FALSE)

p_less <- max(2.2e-16, wilcox_less$p.value)
z_less <- qnorm(p_less, lower.tail = TRUE)

# Calculate Rosenthal's r for each test
r_two_sided <- z_two_sided / sqrt(n_pairs)
r_greater <- z_greater / sqrt(n_pairs)
r_less <- z_less / sqrt(n_pairs)

# Compile tidy output
effect_tbl <- tibble::tibble(
  test_type = c("Two-sided", "Polymarket better (greater)", "LLM better (less)"),
  Z_value = c(z_two_sided, z_greater, z_less),
  N = n_pairs,
  r = c(r_two_sided, r_greater, r_less)
)

kableExtra::kbl(
  effect_tbl,
  digits = 4,
  caption = "Effect sizes (Rosenthal r) for Wilcoxon tests"
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

**Interpretation**\
The Rosenthal r values for the two-sided and one sided "Polymarket better" tests are very close around 0.07, that corresponds to a small effect size.\
With a high number of observations a small effect size becomes statistically significant.\
The direction of the effect sizes is consistent with the previous findings, favoring the Polymarket results, while the gap on individual markets tends to be rather modest when compared to the LLM forecasts.

### 4.3.2 Effect Size Monte carlo simmulation

As the Rosendals r assumes normal distribution, and the data set is not normally distributed, as it has n=14.099 it gives a good estimation.
To further strengthen the effect size evaluation a Monte Carlo simulation was employed.

```{r effect size monte carlo}
set.seed(25)

# Define the difference
brier_diff <- data_test$brier_llm - data_test$brier_market

# Number of simulations
n_sim <- 5000

# Bootstrap the median difference as a proxy for effect magnitude
boot_diffs <- replicate(n_sim, {
  resample <- sample(brier_diff, replace = TRUE)
  median(resample, na.rm = TRUE)
})

# Summarize bootstrap results
boot_summary <- tibble(
  mean_effect = mean(boot_diffs),
  ci_lower = quantile(boot_diffs, 0.025),
  ci_upper = quantile(boot_diffs, 0.975)
)

kableExtra::kbl(
  boot_summary,
  digits = 5,
  caption = "Monte Carlo simulation of effect size (median difference in Brier scores)"
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

**Interpretation**\
The Monte Carlo simmulation created a median diffrence of 0.0625, with an effectively zero-width 95% confidence interval.\
Looking at these results and the results of the Rosendals r values, the simmulation confirms the pattern of Polymarket forecasts outperforming the LLM forecasts by about 0.06 Brier score points on average.
The Monte Carlo bootstrap produces a median difference of 0.0625, with a very narrow confidence interval.
This stability is expected given the size of the dataset.
Together with the Rosenthal r values, the simulation confirms a consistent pattern: Polymarket forecasts outperform the LLM forecasts by roughly 0.06 Brier score points on average.

### 4.3.3 Effect size conclusion

Looking at the effect size results, it shows that overall the performance gap between Polymarket and LLM forecasts is present but small.\
Rosendal's r values all fall around 0.07, which qualifies in the "small effect" range.
This is further confirmed with the Monte Carlo simulation results that indicate that the performance gap between the two forecasts is around 0.06 Brier points favoring the Polymarket forecasts.

## 4.4 Model- and Prompt-Level Wilcoxon Comparisons

This section splits the overall performance results by individual LLM model and prompting strategy.
With the goal of testing whether some LLMs perform significantly closer to the Polymarket than others, and to see if prompting strategies have a significant affect on the forecasting accuracy of the models.

### 4.4.0 Helper fundtions for chapter 4.4

Before we are able to run the model-level and prompt level tests, we need to prepare a few helper functions to enable them to run smoothly.
These functions serve the purpose of standardizing prompt labels, define consistent paring keys, and provide a wrapper for the Wilcoxon test and effect size calculations.

```{r 4.4.0 helpers, include= FALSE}
# --- 4.4.0 HELPERS -----------------------------------------------------------
# Goal:
# - Normalize prompt labels ("CoT" vs "Zero-shot") so joins/pairing are reliable.
# - Define strict pairing keys so every comparison uses the SAME market, date, and (when needed) model/prompt.
# - Provide tiny helper functions for paired Wilcoxon and Rosenthal's r to avoid repeating boilerplate.

# 1) Start from the validated Brier dataset you prepared earlier (data_brier_valid).
#    We keep only a safe copy (data_44) and standardize prompt labels so later
#    pivots/joins don't silently drop rows due to label mismatches.
data_44 <- data_brier_valid %>%
  dplyr::mutate(
    # Canonicalize prompt labels into exactly two buckets used across the thesis:
    #   "Zero-shot" and "CoT".
    prompt_type = dplyr::case_when(
      prompt_type %in% c("Zero-shot","zero","Zero","ZS") ~ "Zero-shot",
      prompt_type %in% c("CoT","cot","Chain-of-Thought","Chain of Thought") ~ "CoT",
      TRUE ~ as.character(prompt_type) # keep anything else as-is (won't be used for prompt tests)
    )
  )

# 2) Define the keys we require to determine "the same observation" for pairing.
#    For model-vs-model tests we pair by (id, date, prompt_type): the event, the day,
#    and the prompt are identical—only the model differs.
#    For pooled prompt tests we'll add `model` into pairing when needed.
pair_keys <- c("id", "date", "prompt_type")

# 3) Helper: Paired Wilcoxon wrapper.
#    - exact = FALSE avoids exact p-value computation (fast and stable for large N).
#    - We suppress warnings to keep knitted output clean.
.wilcox_paired <- function(x, y, alt = "two.sided") {
  suppressWarnings(
    stats::wilcox.test(
      x, y,
      paired = TRUE,
      alternative = alt,
      exact = FALSE,
      conf.int = FALSE
    )
  )
}

# 4) Helper: Rosenthal’s r effect size from a p-value and N pairs.
#    - Converts a two-sided p to a Z and returns r = Z / sqrt(N).
#    - Clamps p away from 0/1 to avoid Inf.
rosenthal_r <- function(p, n) {
  # Guard against p = 0 due to machine precision (use smallest typical R p)
  p <- max(2.2e-16, min(1, p))

  # Convert two-sided p to Z (upper-tail). If p <= .5, use p/2; else use (1 - p/2).
  # This keeps Z positive and aligned with a two-sided test magnitude.
  z <- stats::qnorm(ifelse(p <= 0.5, p / 2, 1 - p / 2), lower.tail = FALSE)

  # Rosenthal's r (effect size for rank-based tests): r = Z / sqrt(N)
  z / sqrt(n)
}
```

```{r helper function biserial correlation, include = FALSE}
# Helper: rank-biserial correlation for paired Wilcoxon signed-rank
rank_biserial_signed <- function(x, y) {
  d <- x - y
  d <- d[is.finite(d) & d != 0]              # Wilcoxon drops zero diffs
  n <- length(d)
  if (n == 0) return(NA_real_)
  wt <- suppressWarnings(wilcox.test(x, y, paired = TRUE, exact = FALSE))
  V  <- as.numeric(wt$statistic)             # sum of positive ranks
  Tt <- n * (n + 1) / 2                      # total rank sum over non-zero pairs
  r_rb <- (2 * V - Tt) / Tt                  # matched-pairs rank-biserial
  return(r_rb)
  
  
}
```

### 4.4.1 Model-level Wilcoxon test

Here we compare the six LLMs agains each other on their respected Brier scores.\
For every pair of models, forecasts are matched on the same market, date, and prompt type, then a paired Wilcoxon test is run on their Brier scores.

-   `median_diff` is defined as Brier(Model A) − Brier(Model B).\
    Negative values mean that **Model A** has lower error and is therefore better.\
-   p-values are adjusted with the Holm method to control for multiple pairwise tests.\
-   `r` reports Rosenthal’s r effect size for each comparison.

```{r Model level wilcoxon test}
# --- 4.4.1 MODEL-LEVEL WILCOXON TESTS ---------------------------------------
# 
# - Test whether some LLMs are statistically better than others (lower Brier).
# - Use paired Wilcoxon on brier_llm, pairing by (id, date, prompt_type).
# - Report median paired difference (Model A - Model B): negative => A better.
# - Adjust p-values (Holm) for multiple pairwise tests.

# 1) Collect the unique model names in a stable order for generating pairs.
models_vec <- data_44 %>%
  dplyr::distinct(model) %>%
  dplyr::arrange(model) %>%
  dplyr::pull()

# 2) All 2-combinations of models we will compare.
pairs <- combn(models_vec, 2, simplify = FALSE)

# 3) For each pair (A,B):
#    - Join the two models on the strict pairing keys (id, date, prompt_type).
#    - Drop non-finite values to be safe.
#    - Run paired Wilcoxon on Brier (A vs B).
#    - Compute median paired difference (A - B) to identify which is better.
#    - Convert p to Rosenthal's r for effect size.
model_pair_results <- purrr::map_dfr(pairs, function(pp) {
  mA <- pp[1]; mB <- pp[2]

  # Subset rows for each model with the columns needed for pairing/testing
  A <- data_44 %>%
    dplyr::filter(model == mA) %>%
    dplyr::select(dplyr::all_of(c(pair_keys, "brier_llm")))

  B <- data_44 %>%
    dplyr::filter(model == mB) %>%
    dplyr::select(dplyr::all_of(c(pair_keys, "brier_llm")))

  # Inner join on the strict keys so we only compare the SAME (id, date, prompt)
  AB <- dplyr::inner_join(
    A %>% dplyr::rename(brier_A = brier_llm),
    B %>% dplyr::rename(brier_B = brier_llm),
    by = pair_keys
  ) %>%
    dplyr::filter(is.finite(brier_A), is.finite(brier_B))

  n_pairs <- nrow(AB)

  # If too few matched pairs, return NA metrics (avoid unstable tests)
  if (n_pairs < 10) {
    return(tibble::tibble(
      model_a    = mA,
      model_b    = mB,
      n_pairs    = n_pairs,
      median_diff= NA_real_,
      V          = NA_real_,
      p          = NA_real_,
      r          = NA_real_,
      better     = NA_character_
    ))
  }

  # Paired Wilcoxon (two-sided) on Brier scores
  wt <- .wilcox_paired(AB$brier_A, AB$brier_B, alt = "two.sided")

  # Median paired difference (A - B): negative means A has LOWER Brier (better)
  med_diff <- stats::median(AB$brier_A - AB$brier_B, na.rm = TRUE)

  # Human-readable label: who is better?
  better_lab <- ifelse(med_diff < 0, mA, mB)

  tibble::tibble(
    model_a     = mA,
    model_b     = mB,
    n_pairs     = n_pairs,
    median_diff = med_diff,                 # < 0 ⇒ model_a better
    V           = unname(wt$statistic),     # Wilcoxon V
    p           = wt$p.value,               # raw p-value
    r           = rosenthal_r(wt$p.value, n_pairs),  # effect size
    better      = better_lab
  )
})

# 4) Multiple-testing correction (Holm) and tidy display order.
model_pair_results <- model_pair_results %>%
  dplyr::mutate(p_holm = p.adjust(p, method = "holm")) %>%
  dplyr::arrange(p_holm, model_a, model_b)

# 5) Neatly formatted table for the report.
kableExtra::kbl(
  model_pair_results,
  digits = 5,
  caption = "Pairwise LLM comparisons on Brier scores (paired by id×date×prompt). Negative median_diff = Model A better."
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

**Interpretation**\
Looking at the paired comparisons between models, a clear pattern is visable, some models consistently outperform others when compared under the same identical market, date and prompting strategies.

**Top-performing models**\
The strongest forecasting LLMs are: - *deepseek_deepseek_r1_free*\
- *google_gemma_3_12b_it*\
- *google_gemma_3_27b_it_free*

These models often produce a negative median_diff value, that indicates that they outperform the other models they are pitted against in the comparison.
While the difference is small in absolute size it is stable across many markets and dates.

**Consistently weakest performer**\
- *openai_gpt_3_5_turbo_0613* - *deepseek_deepseek_chat_v3_0324_free* - *openai_gpt_4o_mini* These models usually score lower than the top-performing models, while generally drawing when comparied agains each other, showing no median_diff.

**Magnitude of differences**\
The performance gaps between strong and weak models fall in the range: - median_diff = 0.02 - 0.04\
meaning the better model reduces average squared error by roughly **2 to 4 percentage points**.

**Statistical reliability**\
Most comparisons remain statistically significant after Holm correction except for deepseek_deepseek_chat_v3_0324_free vs. google_gemma_3_27b_it_free.
The effect sizes (Rosenthal r ≈ 0.15–0.21) indicate **small but consistent** advantages rather than dramatic jumps in performance.

**Overall conclusion** The new reasoning models (Gemma and DeepSeek-R1) consistently outperform older and smaller models.
While these improvements are modest in size, they are highly stable across markets, suggesting that the model choice does have a meaningful impact on forecasting accuracy.


```{r model_vs_market_wilcoxon, message=FALSE, warning=FALSE}
suppressPackageStartupMessages({
  library(dplyr)
  library(kableExtra)
})

# Model vs market implied probabilities on Brier (row-aligned).
# median_diff = LLM − Market (negative = LLM better)

data_44_mm <- data_44 %>%
  dplyr::filter(is.finite(brier_llm), is.finite(yes_price), !is.na(resolution)) %>%
  dplyr::mutate(
    brier_market = (yes_price - resolution)^2,
    diff_brier   = brier_llm - brier_market   # negative => LLM better
  )

model_vs_market_results <- data_44_mm %>%
  dplyr::group_by(model) %>%
  dplyr::summarise(
    n_pairs     = dplyr::n(),
    median_diff = stats::median(diff_brier, na.rm = TRUE),
    V = ifelse(
      n_pairs >= 10,
      unname(wilcox.test(diff_brier, mu = 0, exact = FALSE)$statistic),
      NA_real_
    ),
    p = ifelse(
      n_pairs >= 10,
      wilcox.test(diff_brier, mu = 0, exact = FALSE)$p.value,
      NA_real_
    ),
    r = ifelse(
      is.finite(p),
      rosenthal_r(p, n_pairs),
      NA_real_
    ),
    better = dplyr::case_when(
      !is.finite(median_diff) ~ NA_character_,
      median_diff < 0 ~ "LLM",
      median_diff > 0 ~ "Market",
      TRUE            ~ "Tie"
    ),
    .groups = "drop"
  ) %>%
  dplyr::mutate(p_holm = p.adjust(p, method = "holm")) %>%
  dplyr::arrange(p_holm, model)

kableExtra::kbl(
  model_vs_market_results,
  digits = 5,
  caption = "Model vs market implied probabilities on Brier (row-aligned). median_diff = LLM - Market (negative = LLM better), Holm-adjusted."
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```
### 4.4.2 pooled prompt comparison

In this step, we test if Chain-of-Thought (CoT) prompting produces lower (better) Brier scores than Zero-shot when all models are combined.
To ensure a fair comparison, we only include markets where the same model made a CoT and a Zero-shot prediction on the same date.
By doing this, we isolate the prompting as the only moving variable, and that way are able to only test the difference caused by them.

The paired difference is defined as:

**diff = CoT – Zero-shot**

-   If **diff \< 0** → CoT performs better\
-   If **diff \> 0** → Zero-shot performs better\
-   If **diff = 0** → No difference

A Wilcoxon signed-rank test evaluates whether the median difference is meaningfully different from zero.
I also report two effect sizes: - **r_rosenthal**, which scales the effect relative to sample size\
- **r_rank_biserial**, which reflects the share of pairwise wins after removing ties

```{r prompt level wilcoxon test}
# 4.4.2a POOLED PROMPT COMPARISON (CoT vs Zero-shot)

prompt_pairs_pooled <- data_44 %>%
  dplyr::filter(prompt_type %in% c("CoT", "Zero-shot")) %>%
  dplyr::select(id, date, model, prompt_type, brier_llm) %>%
  tidyr::pivot_wider(names_from = prompt_type, values_from = brier_llm) %>%
  dplyr::filter(is.finite(CoT), is.finite(`Zero-shot`))

n_pooled <- nrow(prompt_pairs_pooled)

wt_pooled <- .wilcox_paired(
  prompt_pairs_pooled$CoT,
  prompt_pairs_pooled$`Zero-shot`,
  alt = "two.sided"
)

median_diff_pooled <- stats::median(
  prompt_pairs_pooled$CoT - prompt_pairs_pooled$`Zero-shot`,
  na.rm = TRUE
)

direction_pooled <- dplyr::case_when(
  median_diff_pooled < 0 ~ "CoT better",
  median_diff_pooled > 0 ~ "Zero-shot better",
  TRUE                  ~ "No meaningful difference"
)

r_rb_pooled <- rank_biserial_signed(
  prompt_pairs_pooled$CoT,
  prompt_pairs_pooled$`Zero-shot`
)

prompt_pooled_tbl <- tibble::tibble(
  n_pairs     = n_pooled,
  median_diff = median_diff_pooled,
  direction   = direction_pooled,
  V           = unname(wt_pooled$statistic),
  p_value     = wt_pooled$p.value,
  r_rosenthal = rosenthal_r(wt_pooled$p.value, n_pooled),
  r_rank_biserial = r_rb_pooled
)

# ---- Presentation layer ----
col_labels <- c(
  n_pairs          = "N pairs",
  median_diff      = "Median Δ Brier",
  V                = "Wilcoxon V",
  p_value          = "p-value",
  r_rosenthal      = "Rosenthal r",
  r_rank_biserial  = "Rank-biserial r"
)

rename_map <- stats::setNames(names(col_labels), col_labels)

prompt_pooled_tbl_pretty <- prompt_pooled_tbl %>%
  dplyr::select(-direction) %>%        # REMOVE direction column
  dplyr::rename(!!!rename_map)

kableExtra::kbl(
  prompt_pooled_tbl_pretty,
  digits = 5,
  caption = "Pooled CoT vs Zero-shot (paired). Δ = CoT − Zero-shot. Rosenthal r = Z/√N."
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

**Interpretation**\
The table shows that the median diffrence between the two prompting styles is zero, that means that neither prompt style shows a clear performance advantage at the paired sample level.

The Wilcoxon test does reach statistical significance (p ≈ 0.0003), but both effect size measures are extremely small:

-   **r_rosenthal ≈ 0.04** → a very small effect favouring Zero-Shot
-   **r_rank_biserial ≈ –0.08** → nearly an even split between CoT wins and Zero-shot wins, favoring CoT ever slightly.

In practical terms, CoT and Zero-shot prompting perform almost identically when averaged across all models.

### 4.4.3 Per-Model Prompt Comparison

In this step, we investigate if CoT prompting improves forecasting accuracy for individual models, rather than all models combined like we did in 4.4.2.\
For each individual model we run a paired Wilcoxon signed-rank test on the difference\
**diff = CoT – Zero-shot**, where negative values mean CoT has the lower (better) Brier score.\
On top of that we also compute the effect size measures:\
**r_rosenthal**, which captures the standardized magnitude of the difference\
**r_rank_biserial**, which shows whether CoT or Zero-shot wins more of the paired comparisons after removing ties

Holm correction is applied to the p-values to account for testing multiple models.

```{r per model prompt effect}
# Paired CoT vs Zero-shot comparison per model.

per_model_pairs <- data_44 %>%
  dplyr::filter(prompt_type %in% c("CoT", "Zero-shot")) %>%
  dplyr::select(id, date, model, prompt_type, brier_llm) %>%
  tidyr::pivot_wider(names_from = prompt_type, values_from = brier_llm) %>%
  dplyr::filter(is.finite(CoT), is.finite(`Zero-shot`)) %>%
  dplyr::mutate(diff = CoT - `Zero-shot`)   # negative = CoT lower Brier (better)

per_model_results <- per_model_pairs %>%
  dplyr::group_by(model) %>%
  dplyr::summarise(
    n_pairs = dplyr::n(),
    median_diff = stats::median(diff, na.rm = TRUE),
    mean_diff = mean(diff, na.rm = TRUE),
    V = ifelse(
      n_pairs >= 10,
      .wilcox_paired(CoT, `Zero-shot`, alt = "two.sided")$statistic,
      NA_real_
    ),
    p_value = ifelse(
      n_pairs >= 10,
      .wilcox_paired(CoT, `Zero-shot`, alt = "two.sided")$p.value,
      NA_real_
    ),
    r_rank_biserial = ifelse(
      n_pairs >= 10,
      rank_biserial_signed(CoT, `Zero-shot`),
      NA_real_
    ),
    .groups = "drop"
  ) %>%
  dplyr::mutate(
    direction = dplyr::case_when(
      median_diff < 0 ~ "CoT better",
      median_diff > 0 ~ "Zero-shot better",
      TRUE            ~ "No clear difference"
    ),
    r_rosenthal = ifelse(
      is.finite(p_value),
      rosenthal_r(p_value, n_pairs),
      NA_real_
    ),
    p_value_holm = p.adjust(p_value, method = "holm")
  ) %>%
  dplyr::arrange(p_value_holm, model)

# ---- Presentation layer ----
col_labels <- c(
  model            = "Model",
  n_pairs          = "N pairs",
  median_diff      = "Median Δ Brier",
  mean_diff        = "Mean Δ Brier",
  V                = "Wilcoxon V",
  p_value          = "p-value",
  p_value_holm     = "Holm-adjusted p",
  r_rosenthal      = "Rosenthal r",
  r_rank_biserial  = "Rank-biserial r"
)

rename_map <- stats::setNames(names(col_labels), col_labels)

per_model_results_pretty <- per_model_results %>%
  dplyr::select(-direction) %>%      # REMOVE direction column
  dplyr::rename(!!!rename_map)

kableExtra::kbl(
  per_model_results_pretty,
  digits = 5,
  caption = "Per-model CoT vs Zero-shot (paired). Δ = CoT − Zero-shot; negative favors CoT."
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

**Interpretation**

Looking at the table here in 4.4.3, the median difference is exactly zero.
Meaning that neither prompting strategy consistently produces lower Brier scores.
There is a small but noticeable mean difference (both a negative and a positive depending on model) show a minuscule shift, while non of them point towards a meaningful practical advantage.

The p-values indicate that there is a statistically significant difference to the first 4 models in the table.

After applying Holm correction, **three models still show statistically significant differences**:

-   **deepseek_deepseek_r1_free**\
-   **openai_gpt_3_5_turbo_0613**\
-   **google_gemma_3_12b_it**

The rank-biserial effect sizes reinforce this:\
- Values around **−0.26** for `deepseek_r1_free` and `gpt-3.5-turbo` indicate a slight tendency for CoT to produce lower errors,\
- while positive values **+0.25** for `gemma_3_12b_it` suggest a slight tilt toward Zero-shot.

However, these signs are not consistent among the models, and all effect sizes are small in absolute magnitude.\
Rosenthal’s r values (≈0.17–0.24) similarly indicate **small effects**, consistent with very minor differences in a practical setting.

Two models show no statistical difference at all after Holm correction are: - **google_gemma_3_27b_it_free**\
- **openai_gpt_4o_mini**

Overall, the results show that: 
- Prompting style does not meaningfully change forecasting performance.\
- Small statistical differences appear in some models, but effect sizes are weak and inconsistent.\
- Neither prompt type can be considered reliably better across models.


**Summary of 4.4.2** Across all models and prompt configurations, CoT prompting does not reliably improve forecasting accuracy.
Differences that are statistically detectable are small in magnitude and do not translate into practical performance gains.

#### 4.4.3 Effect Size Interpretation

Across all prompt-level comparisons, the estimated effect sizes are consistently small.
Rosenthal's r ranges from 0.19-0.24, that falls under a commonly used benchmark for a small effect.\
The rank-biserial coefficients range from -0.26-0.04, that indicates that selection of prompt styles doesn't systematically effect the prediction accuracy of the forecasts.\
The combination of statistically significant p-values and very small effect sizes is explained by the large number of observations in the dataset.
As we have thousands of data points, even minor differences are detectable statistically, while these differences are not big enough to be economically significant, in other words large enough to matter in practice. Together these results indicate that Chain-of-Thought prompting does not meaningfully improve the accuracy of probabilistic LLM forecasting.
Even when evaluated under controlled, paired conditions.
Any observed differences are stable but very small.
Meaning that prompt choose does not meaningfully effect model performance in this setting.

# Chapter 5



## 5.1 Assign volume group using each market’s final observation

To investigate whether forecasting performance depends on market size, each market is assigned to a volume group based on its **final observed trading volume**.
For each market we take the last date on which a `volume_num` is recorded and than treat that as the market *end-state* trading volume.
This approach allows for a comparison of the market liquidity while it assures that liquidity/volume measurements are comparable across markets.

We then compute tertiles of the final volumes and use these cut-offs to define three liquidity groups.

-   **Low volume** - markets in the bottom third of the final-volume distribution\
-   **Mid volume** - markets in the middle third of the distribution\
-   **High volume** - markets in the top third of the distribution

Finally, we report how many markets fall into each band and how many **paired CoT vs. Zero-shot observations** are available within each band.

```{r final voluma evaluation, message=FALSE, warning=FALSE}
# 5.0 FINAL volume groupS PER MARKET (Method A: last finite positive volume_num)

suppressPackageStartupMessages({
  library(dplyr)
  library(tidyr)
  library(kableExtra)
})

# 0) Ensure date columns are stored as Date objects
data_brier_valid <- data_brier_valid %>%
  mutate(
    date     = as.Date(date),
    end_date = suppressWarnings(as.Date(end_date))
  )

# 1) For each market (id), take the LAST row with a finite, positive volume_num
last_with_volume <- data_brier_valid %>%
  filter(is.finite(volume_num), volume_num > 0) %>%
  group_by(id) %>%
  slice_max(order_by = date, n = 1, with_ties = FALSE) %>%
  ungroup() %>%
  select(
    id,
    question,
    last_date    = date,
    volume_final = volume_num
  )

# 2) One row per market, with final volume attached when available
market_final <- data_brier_valid %>%
  distinct(id, question) %>%
  left_join(last_with_volume, by = c("id", "question"))

# 3) Compute tercile cut-offs on strictly positive final volumes
cuts <- market_final %>%
  filter(is.finite(volume_final), volume_final > 0) %>%
  summarise(
    cut_33 = quantile(volume_final, 0.33, na.rm = TRUE),
    cut_67 = quantile(volume_final, 0.67, na.rm = TRUE),
    .groups = "drop"
  )

# 4) Assign volume groups (Low / Mid / High / Unknown)
market_final <- market_final %>%
  mutate(
    volume_group = case_when(
      is.finite(volume_final) & volume_final > 0 & volume_final <= cuts$cut_33           ~ "Low volume",
      is.finite(volume_final) & volume_final > cuts$cut_33 & volume_final <= cuts$cut_67 ~ "Mid volume",
      is.finite(volume_final) & volume_final > cuts$cut_67                               ~ "High volume",
      TRUE                                                                               ~ "Unknown"
    ),
    volume_group = factor(
      volume_group,
      levels = c("Low volume", "Mid volume", "High volume", "Unknown")
    )
  )

# 5) Number of markets in each band
market_counts <- market_final %>%
  count(volume_group) %>%
  mutate(share = round(100 * n / sum(n), 1))

kbl(
  market_counts,
  caption = "Number of markets per final-volume group (based on last finite positive volume_num).",
  digits = 1
) %>%
  kable_styling(full_width = FALSE)

# 6) Number of paired CoT vs Zero-shot rows per band
#    (note: 'CoT' is stored as 'Chain-of-Thought' in prompt_type)
band_pair_counts <- data_brier_valid %>%
  inner_join(market_final %>% select(id, volume_group), by = "id") %>%
  filter(prompt_type %in% c("Zero-shot", "Chain-of-Thought")) %>%
  select(id, date, model, prompt_type, brier_llm, volume_group) %>%
  pivot_wider(names_from = prompt_type, values_from = brier_llm) %>%
  filter(is.finite(`Zero-shot`), is.finite(`Chain-of-Thought`)) %>%
  count(volume_group, name = "n_pairs")

kbl(
  band_pair_counts,
  caption = "Number of paired Chain-of-Thought vs Zero-shot observations by final-volume group.",
  digits = 0
) %>%
  kable_styling(full_width = FALSE)

# 7) Markets that remain 'Unknown' (for manual inspection)
unknown_markets <- market_final %>%
  filter(volume_group == "Unknown") %>%
  left_join(
    data_brier_valid %>%
      group_by(id) %>%
      summarise(
        max_date_overall    = max(date, na.rm = TRUE),
        any_positive_volume = any(is.finite(volume_num) & volume_num > 0),
        .groups = "drop"
      ),
    by = "id"
  ) %>%
  arrange(question)

kbl(
  unknown_markets %>%
    select(
      id, question, last_date, volume_final,
      any_positive_volume, max_date_overall
    ),
  caption = "Markets flagged as Unknown (no finite positive final volume_num observed).",
  digits = 0
) %>%
  kable_styling(full_width = FALSE)

# 8) Convenience listing used later (e.g., joins in Section 5.1)
market_band_listing <- market_final %>%
  arrange(volume_group, desc(volume_final)) %>%
  select(id, question, volume_final, volume_group)
```

***Interpretation***

Final trading volumes split almost evenly across the three groups, with three markets that are classified as **Unknown**.

The three Unknown markets appear to be cases of missing volume data rather than true zero-volume markets.
These are dealt with in chapter 5.1.1.

### 5.1.1 Fixing unknown values

After manually inspecting the three markets that had a unknown value assigned to them at the end date, a manual inspection of the Polymarket was conducted to look up those volume values.
The values are assigned to their corresponding markets here below.

```{r manual volume update, message=FALSE, warning=FALSE}
# 5.0.b MANUAL FIX OF FINAL VOLUMES AND REBANDING

manual_volumes <- tibble::tribble(
  ~id,     ~volume_final,
   565367,  782089,
   563621,  226948,
   513304, 1443302
)

# Update market_final with manual volumes (manual values override original)
market_final <- market_final %>%
  dplyr::left_join(manual_volumes, by = "id", suffix = c("", ".manual")) %>%
  dplyr::mutate(
    volume_final = dplyr::coalesce(volume_final.manual, volume_final)
  ) %>%
  dplyr::select(-dplyr::any_of("volume_final.manual"))

# Recompute cut points on valid final volumes
cuts <- market_final %>%
  dplyr::filter(is.finite(volume_final), volume_final > 0) %>%
  dplyr::summarise(
    cut_33 = stats::quantile(volume_final, 0.33, na.rm = TRUE),
    cut_67 = stats::quantile(volume_final, 0.67, na.rm = TRUE),
    .groups = "drop"
  )

# Re-assign volume groups using updated cut points
market_final <- market_final %>%
  dplyr::mutate(
    volume_group = dplyr::case_when(
      is.finite(volume_final) & volume_final > 0 & volume_final <= cuts$cut_33           ~ "Low volume",
      is.finite(volume_final) & volume_final > cuts$cut_33 & volume_final <= cuts$cut_67 ~ "Mid volume",
      is.finite(volume_final) & volume_final > cuts$cut_67                               ~ "High volume",
      TRUE                                                                               ~ "Unknown"
    ),
    volume_group = factor(
      volume_group,
      levels = c("Low volume", "Mid volume", "High volume", "Unknown")
    )
  )

# Compute max valid volume for the range label
max_vol <- market_final %>%
  dplyr::filter(is.finite(volume_final), volume_final > 0) %>%
  dplyr::summarise(max_vol = max(volume_final, na.rm = TRUE), .groups = "drop") %>%
  dplyr::pull(max_vol)

# Quick check of market counts after manual fixes (presentation-only relabeling)
market_counts <- market_final %>%
  dplyr::count(volume_group) %>%
  dplyr::mutate(
    share = round(100 * n / sum(n), 1),

    # relabel group names for presentation only
    volume_group = dplyr::recode(
      volume_group,
      "Low volume"  = "Low liquidity",
      "Mid volume"  = "Medium liquidity",
      "High volume" = "High liquidity"
    ),

    # add a readable range column (uses the actual cutpoints)
`Volume range` = dplyr::case_when(
  volume_group == "Low liquidity"    ~ paste0("0 – ", scales::comma(round(cuts$cut_33, 0))),
  volume_group == "Medium liquidity" ~ paste0(
    scales::comma(round(cuts$cut_33, 0)), " – ", scales::comma(round(cuts$cut_67, 0))
  ),
  volume_group == "High liquidity"   ~ paste0(
    scales::comma(round(cuts$cut_67, 0)), " – ", scales::comma(round(max_vol, 0))
  ),
  TRUE                               ~ NA_character_
)
  ) %>%
  dplyr::rename(`Liquidity group` = volume_group) %>%
  dplyr::select(`Liquidity group`, `Volume range`, n, share)

kableExtra::kbl(
  market_counts,
  caption = "Markets per final-liquidity group after manual volume corrections.",
  digits = 1
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

***Interpretation***

After applying the manual correction, all markets have a valid final volume, allowing for a balanced distirbution across Low, Mid and High groups.

### 5.1.2 Building stage snapshots with volume groups

In this step we create a stage-level dataset, that will be used in all future volume-group analysis.
Starting from the validated brier dataset we attach each market's `volume_group` (Low, Mid, High) to every daily observation.

For every combination of market, model and prompting strategy (`id × model × prompt_type`), we identify three reference points in the timeline: **first**, **mid**, and **last** observation.

At each selected point, we compute the paired difference

$$
\text{brier\_diff} = \text{Brier}_{\text{LLM}} - \text{Brier}_{\text{Polymarket}},
$$

The resulting `temporal_vol` dataset contains one row per\
`id × model × prompt_type × stage`,\
with the appropriate volume group attached.

```{r bulid stage data w volume, message=FALSE, warning=FALSE}
# 5.1.2 BUILD STAGE DATA WITH VOLUME groups

suppressPackageStartupMessages({
  library(dplyr)
  library(tidyr)
})

# Attach the (corrected) volume group from market_final to every row
data_vol <- data_brier_valid %>%
  inner_join(market_final %>% select(id, volume_group), by = "id")

# Helper: coerce resolution to numeric 0/1 (re-stated here for clarity)
coerce_resolution01 <- function(x) {
  if (is.logical(x)) return(as.integer(x))
  if (is.numeric(x)) return(as.numeric(x))
  if (is.character(x)) {
    xu <- toupper(trimws(x))
    return(dplyr::case_when(
      xu %in% c("YES", "TRUE", "1", "Y") ~ 1,
      xu %in% c("NO", "FALSE", "0", "N") ~ 0,
      TRUE                               ~ NA_real_
    ))
  }
  as.numeric(x)
}

# Safety: ensure Brier columns are present and consistent
data_vol <- data_vol %>%
  mutate(
    resolution01 = coerce_resolution01(resolution),
    forecast     = as.numeric(forecast),
    yes_price    = as.numeric(yes_price),
    brier_llm    = ifelse(is.finite(brier_llm),
                          brier_llm,
                          (forecast - resolution01)^2),
    brier_market = ifelse(is.finite(brier_market),
                          brier_market,
                          (yes_price - resolution01)^2)
  )

# First and last dates per id × model × prompt_type
points_vol <- data_vol %>%
  group_by(id, model, prompt_type) %>%
  summarise(
    first_date = min(date, na.rm = TRUE),
    last_date  = max(date, na.rm = TRUE),
    .groups    = "drop"
  ) %>%
  mutate(
    mid_date = first_date +
      floor(as.numeric(difftime(last_date, first_date, units = "days")) / 2)
  ) %>%
  tidyr::pivot_longer(
    c(first_date, mid_date, last_date),
    names_to  = "stage_raw",
    values_to = "target_date"
  ) %>%
  mutate(
    stage = dplyr::recode(
      stage_raw,
      "first_date" = "first",
      "mid_date"   = "mid",
      "last_date"  = "last"
    )
  ) %>%
  select(id, model, prompt_type, stage, target_date)

# Nearest observed row to each target date, per stage
temporal_vol <- data_vol %>%
  inner_join(points_vol, by = c("id", "model", "prompt_type")) %>%
  mutate(
    abs_diff_days = abs(as.numeric(difftime(date, target_date, units = "days")))
  ) %>%
  group_by(id, model, prompt_type, stage) %>%
  slice_min(abs_diff_days, n = 1, with_ties = FALSE) %>%
  ungroup() %>%
  filter(is.finite(brier_llm), is.finite(brier_market)) %>%
  mutate(
    brier_diff = brier_llm - brier_market  # >0: market more accurate
  )
```



### 5.1.3 Stage summaries by volume group

To better understand how LLM accuracy compares to that of the Polymarket at different points in time, we summarize the stage-level differences separately for the Low, Mid and High volume groups.

To understand how LLM accuracy compares to Polymarket at different points in time, we summaries the stage-level differences separately for the Low, Mid, and High volume groups.
For each market and stage, we merge the model and prompt observations into one market-level value.

$$
\text{diff\_market} = \text{median}(\text{brier\_diff}),
$$

After that we calculate the summary statistics-median, mean and standard deviation, along with number of markets for each volume group and stage.

```{r volume group x stage}
# 5.1.3 (revised, Way A): Market-level summaries
# One value per market per stage: median of brier_diff across models/prompts

market_stage <- temporal_vol %>%
  group_by(volume_group, id, stage) %>%
  summarise(
    diff_market = median(brier_diff, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(is.finite(diff_market))

stage_summary_vol_market <- market_stage %>%
  group_by(volume_group, stage) %>%
  summarise(
    n_markets    = n_distinct(id),
    median_diff  = median(diff_market, na.rm = TRUE),
    mean_diff    = mean(diff_market, na.rm = TRUE),
    sd_diff      = sd(diff_market, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(
    factor(volume_group, levels = c("Low volume","Mid volume","High volume","Unknown")),
    factor(stage, levels = c("first","mid","last"))
  )

kableExtra::kbl(
  stage_summary_vol_market,
  digits = 4,
  caption = "Market-level LLM − Polymarket Brier by volume group and stage. Positive means Polymarket lower error."
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

***Interpretation***

From the table in chapter 5.1.3 we can see that the differences are positive across all stages, indicating that the Polymarket is consistently more accurate than the LLM forecasts on average.
The size of the gap varies a bit by volume groups.
Low volume groups show a smaller mean difference than their Mid- and High counter parts, this suggests that the Polymarket has a bigger advantage in more liquid markets.

Looking at the stages (first, mid, last), there is no clear winner, although mid outperforms the other two in the Low and Mid groups.
Over all these results indicate a persistent accuracy advantage for the Polymarket across volume groups and time stages.

### 5.1.4 Wilcoxon by volume group and stage

Here we formally compare forecasting accuracy between the LLMs and Polymarket at the **Market level**, for each stage.

For each **volume group × stage** combination, we run a Wilcoxon signed-rank test where the median differs from zero.

```{r power volume group x stage, message=FALSE, warning=FALSE}
# 5.1.4 Inference: Wilcoxon per volume group × stage using market-level data

wilcox_results_band_stage <- market_stage %>%
  group_by(volume_group, stage) %>%
  summarise(
    n_markets   = n_distinct(id),
    median_diff = median(diff_market, na.rm = TRUE),
    mean_diff   = mean(diff_market, na.rm = TRUE),
    sd_diff     = sd(diff_market, na.rm = TRUE),
    V = tryCatch(
      wilcox.test(diff_market, mu = 0, exact = FALSE)$statistic,
      error = function(e) NA
    ),
    p_value = tryCatch(
      wilcox.test(diff_market, mu = 0, exact = FALSE)$p.value,
      error = function(e) NA
    ),
    r_rosenthal = tryCatch(
      rosenthal_r(
        wilcox.test(diff_market, mu = 0, exact = FALSE)$p.value,
        n_distinct(id)
      ),
      error = function(e) NA
    ),
    r_rank_biserial = tryCatch(
      rank_biserial_signed(diff_market, rep(0, length(diff_market))),
      error = function(e) NA
    ),
    direction = case_when(
      median_diff < 0 ~ "LLM better",
      median_diff > 0 ~ "Market better",
      TRUE            ~ "No meaningful difference"
    ),
    .groups = "drop"
  ) %>%
  arrange(
    factor(volume_group, levels = c("Low volume","Mid volume","High volume","Unknown")),
    factor(stage, levels = c("first","mid","last"))
  )

# ---- Presentation layer: manually set column labels here ----
col_labels <- c(
  volume_group     = "Liquidity group",
  stage            = "Market stage",
  n_markets        = "N markets",
  median_diff      = "Median Δ Brier",
  mean_diff        = "Mean Δ Brier",
  sd_diff          = "SD Δ Brier",
  V                = "Wilcoxon V",
  p_value          = "p-value",
  r_rosenthal      = "Rosenthal r",
  r_rank_biserial  = "Rank-biserial r",
  direction        = "Direction"
)

# rename() needs: new = old, so we invert the mapping
rename_map <- stats::setNames(names(col_labels), col_labels)

wilcox_results_band_stage_pretty <- wilcox_results_band_stage %>%
  dplyr::rename(!!!rename_map)

kableExtra::kbl(
  wilcox_results_band_stage_pretty,
  digits = 4,
  caption = "Wilcoxon signed-rank results (market-level) by liquidity group and stage."
) %>%
  kableExtra::kable_styling(full_width = FALSE)

```

***Interpretation***

Looking at the results of the Wilcoxon signed rank test for volume group and stage, it is clear that the **Polymarket is more accurate than the LLM forecasts** in all stages and liquidity levels.
This is highlighted by a positive median and mean values along with a significant p-values (all $p < 0.01$).

The effect sizes are large throughout the data set,(Rosenthal’s $r \approx 0.66$- 0.87; rank-biserial correlations mostly greater than 0.80), this implies a considerable performance advantage for the Polymarket.
The performance gap is slightly greater in the Mid-High volume markets, while the direction stays consistent across time and liquidity stages of the markets.

### 5.2-power Overall power by volume group

To check if each volume group has enough markets to achieve a reliable conclusion.
We combine stage-level results within each gropu and estimate the Wilcoxon power through bootstrapping.

```{r over all power by volume}
### 5.2 Overall Power by volume group (Market-Level, Stage-Agnostic)

# We start from market_stage (market-level differences)
# Collapse across stage by taking median difference per market
market_band_overall <- market_stage %>%
  group_by(id, volume_group) %>%
  summarise(
    diff_market = median(diff_market, na.rm = TRUE),
    .groups = "drop"
  )

# Bootstrap power function (same logic as 5.1.d)
bootstrap_power <- function(x, nboot = 2000, alpha = 0.05) {
  boot_p <- replicate(nboot, {
    x_boot <- sample(x, replace = TRUE)
    suppressWarnings(wilcox.test(x_boot, mu = 0, exact = FALSE)$p.value)
  })
  mean(boot_p < alpha)
}

# Compute power per volume group
power_band_overall <- market_band_overall %>%
  group_by(volume_group) %>%
  summarise(
    n_markets   = n_distinct(id),
    power_est   = bootstrap_power(diff_market),
    .groups = "drop"
  ) %>%
  arrange(factor(volume_group, levels = c("Low volume","Mid volume","High volume","Unknown")))

kableExtra::kbl(
  power_band_overall,
  digits = 3,
  caption = "Bootstrap-estimated power by volume group (market-level, pooled across stages)."
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

***Interpretation***\
Looking at the table we can see that all groups show a high statistical power (≥0.93), this indicates that there are enough markets to reliably detect differences between in the data between the polymarket and LLM forecasts.
This confirms that the conclusions in 5.1.4 are based on comparisons with enough power.

### 5.3 Does the performance gap differ across volume groups at each stage?

In this step we test if the market-level performance gap between LLMs and Polymarket changes across the volume groups at each time stage.
For each stage we apply the Kruskal-Wallis test to the market-level differences `diff_market` across volume groups.
If the overall test is significant, we apply a Dunn's test with a Holm correction for a pairwise comparison to see if volume groups differ one from another.

```{r performance at diff volume, message=FALSE, warning=FALSE}

# 5.3a Does the performance gap differ across liquidity (volume) groups at each stage?
# - Main text: Kruskal–Wallis by stage + compact post-hoc Dunn summary (filtered)
# - Appendix: Full Dunn tables (all pairwise comparisons)

library(dplyr)
library(stringr)
library(kableExtra)
library(FSA)
library(tibble)

# ----------------------------
# 0) Robust tidy labels (case-insensitive; avoids accidental NA factors)
# ----------------------------
market_stage_tidy <- market_stage %>%
  mutate(
    stage_raw = tolower(as.character(stage)),
    vol_raw   = tolower(as.character(volume_group)),
    stage = factor(stage_raw,
      levels = c("first", "mid", "last"),
      labels = c("First", "Mid", "Last")
    ),
    volume_group = factor(vol_raw,
      levels = c("low", "mid", "high"),
      labels = c("Low", "Mid", "High")
    )
  ) %>%
  select(-stage_raw, -vol_raw)

# ----------------------------
# 1) Kruskal–Wallis test per stage (defensive: requires >= 2 groups)
# ----------------------------
kw_by_stage <- market_stage_tidy %>%
  group_by(stage) %>%
  summarise(
    `N (market-stage obs.)` = n(),
    `# liquidity groups`    = n_distinct(volume_group[!is.na(volume_group)]),
    `Kruskal–Wallis p-value` = if (`# liquidity groups` >= 2) {
      kruskal.test(diff_market ~ volume_group)$p.value
    } else {
      NA_real_
    },
    .groups = "drop"
  )

kableExtra::kbl(
  kw_by_stage,
  digits = 4,
  caption = "Kruskal–Wallis tests of LLM–Polymarket Brier score differences across liquidity groups, by market stage (market-level). Stages with <2 liquidity groups are reported as NA."
) %>%
  kableExtra::kable_styling(full_width = FALSE)

# ----------------------------
# 2) Dunn post-hoc tests per stage (Holm-adjusted; defensive)
# ----------------------------
dunn_full <- market_stage_tidy %>%
  group_by(stage) %>%
  group_modify(~{
    ng <- n_distinct(.x$volume_group[!is.na(.x$volume_group)])
    if (ng < 2) {
      return(tibble(
        Comparison = NA_character_,
        Z          = NA_real_,
        P.unadj    = NA_real_,
        P.adj      = NA_real_
      ))
    }
    FSA::dunnTest(diff_market ~ volume_group, data = .x, method = "holm")$res
  }) %>%
  ungroup() %>%
  mutate(
    Comparison = str_replace_all(Comparison, " - ", " vs ")
  )

# Compact / main-text version: only show potentially relevant comparisons (p_adj < 0.10)
dunn_compact_main <- dunn_full %>%
  filter(!is.na(P.adj)) %>%
  transmute(
    `Market stage` = stage,
    `Comparison`   = Comparison,
    `Z`            = Z,
    `Holm-adjusted p-value` = P.adj,
    `Sig.` = case_when(
      P.adj < 0.05 ~ "*",
      P.adj < 0.10 ~ "†",
      TRUE ~ ""
    )
  ) %>%
  filter(`Holm-adjusted p-value` < 0.10) %>%
  arrange(`Market stage`, `Holm-adjusted p-value`)

kableExtra::kbl(
  dunn_compact_main,
  digits = 4,
  caption = "Post-hoc Dunn tests (Holm-adjusted) within each market stage. For readability, only comparisons with Holm-adjusted p < 0.10 are shown (* p < 0.05, † p < 0.10). Full results are reported in Appendix X."
) %>%
  kableExtra::kable_styling(full_width = FALSE)

# ----------------------------
# 3) Appendix: full Dunn output (all comparisons)
# ----------------------------
dunn_table_appendix <- dunn_full %>%
  transmute(
    `Market stage` = stage,
    `Comparison`   = Comparison,
    `Z`            = Z,
    `Unadjusted p-value` = P.unadj,
    `Holm-adjusted p-value` = P.adj
  ) %>%
  arrange(`Market stage`, `Holm-adjusted p-value`)

kableExtra::kbl(
  dunn_table_appendix,
  digits = 4,
  caption = "Appendix X: Full post-hoc Dunn tests (Holm-adjusted) comparing liquidity groups within each market stage (market-level)."
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

***Interpretation***\
Across the stages, the Kruskal-Wallis test results show little evidence that the LLM-Polymarket accuracy gap varies systematically across volume groups.
The test are not significant for the **first** and **last** stages, this indicates no difference between Low, Mid and high volume markets at these points in time.

The **Mid** stage shows a statistically significant effect ($p \approx 0.026$), the followup Dunn test reveals that none of the pairwise differences stay significant after the Holm correction.

<!-- ### 5.4.a Prepare the plotting  -->

<!-- ```{r prep plotting data} -->
<!-- # --- 5.4.a Build a per-market day index -------------------------------------- -->
<!-- # We create a mapping (id, date) -> day_index so every model uses the same x-axis. -->
<!-- # Day 1 is the earliest date observed for that market; Day 2 the next unique date, etc. -->

<!-- # 1) Make sure dates are proper Dates -->
<!-- if (!inherits(data_vol$date, "Date")) { -->
<!--   data_vol <- data_vol %>% -->
<!--     dplyr::mutate( -->
<!--       date = lubridate::parse_date_time( -->
<!--         date, -->
<!--         orders = c("Y-m-d","d-m-Y","d/m/Y","Ymd","dmY","mdY"), -->
<!--         tz = "UTC" -->
<!--       ) |> as.Date() -->
<!--     ) -->
<!-- } -->

<!-- # 2) Build the (id, date) -> day_index map (unique dates per market) -->
<!-- day_map <- data_vol %>% -->
<!--   dplyr::distinct(id, date) %>% -->
<!--   dplyr::arrange(id, date) %>% -->
<!--   dplyr::group_by(id) %>% -->
<!--   dplyr::mutate(day_index = dplyr::row_number()) %>% -->
<!--   dplyr::ungroup() -->

<!-- # 3) Join the map back; keep everything else intact -->
<!-- plot_data_idx <- data_vol %>% -->
<!--   dplyr::left_join(day_map, by = c("id","date")) %>% -->
<!--   dplyr::mutate( -->
<!--     model_label = paste(model, prompt_type, sep = " / "), -->
<!--     polymarket  = as.numeric(yes_price), -->
<!--     forecast    = as.numeric(forecast), -->
<!--     resolution01 = as.numeric(resolution01) # already prepared earlier in your pipeline -->
<!--   ) -->
<!-- ``` -->

<!-- #### 5.4.b A function to draw a single market timeline -->

<!-- ```{r draw single market} -->
<!-- # --- 5.4.b Plot function (x = day_index) ------------------------------------- -->
<!-- # Shows: -->
<!-- # - Polymarket price (black) -->
<!-- # - Each model/prompt forecast (colored) -->
<!-- # - Horizontal dashed red line at resolution (0 or 1) if known -->

<!-- plot_market_idx <- function(market_id) { -->
<!--   df <- plot_data_idx %>% dplyr::filter(id == !!market_id) -->

<!--   # Resolve the market-level resolution if present (prefer last non-NA) -->
<!--   res_line <- df %>% -->
<!--     dplyr::filter(is.finite(resolution01)) %>% -->
<!--     dplyr::arrange(date) %>% -->
<!--     dplyr::slice_tail(n = 1) %>% -->
<!--     dplyr::pull(resolution01) -->
<!--   res_line <- ifelse(length(res_line) == 1, res_line, NA_real_) -->

<!--   # Build long data for plotting (multiple LLM lines + one PM line) -->
<!--   llm_lines <- df %>% -->
<!--     dplyr::select(day_index, model_label, forecast) %>% -->
<!--     dplyr::rename(value = forecast) %>% -->
<!--     dplyr::filter(is.finite(value)) %>% -->
<!--     dplyr::mutate(series = "LLM") -->

<!--   pm_line <- df %>% -->
<!--     dplyr::select(day_index, polymarket) %>% -->
<!--     dplyr::distinct() %>% -->
<!--     dplyr::rename(value = polymarket) %>% -->
<!--     dplyr::filter(is.finite(value)) %>% -->
<!--     dplyr::mutate(model_label = "Polymarket", series = "PM") -->

<!--   plt_df <- dplyr::bind_rows(llm_lines, pm_line) -->

<!--   # X ticks: every 7 days if long; otherwise pretty() -->
<!--   x_min <- min(plt_df$day_index, na.rm = TRUE) -->
<!--   x_max <- max(plt_df$day_index, na.rm = TRUE) -->
<!--   step  <- if ((x_max - x_min) >= 21) 7 else 1 -->

<!--   ggplot(plt_df, aes(x = day_index, y = value)) + -->
<!--     geom_line( -->
<!--       data = subset(plt_df, series == "PM"), -->
<!--       linewidth = 1.2, color = "black" -->
<!--     ) + -->
<!--     geom_line( -->
<!--       data = subset(plt_df, series == "LLM"), -->
<!--       aes(color = model_label), -->
<!--       linewidth = 0.7, alpha = 0.9 -->
<!--     ) + -->
<!--     # Resolution line if available -->
<!--     { if (is.finite(res_line)) geom_hline(yintercept = res_line, linetype = "dashed", color = "red") } + -->
<!--     scale_y_continuous(limits = c(0,1), breaks = seq(0,1,by = 0.1)) + -->
<!--     scale_x_continuous( -->
<!--       breaks = seq(x_min, x_max, by = step), -->
<!--       expand = expansion(mult = c(0.01, 0.02)) -->
<!--     ) + -->
<!--     labs( -->
<!--       title = paste0(unique(df$question)[1]), -->
<!--       subtitle = paste0("Market ", market_id, " — LLM forecasts vs Polymarket (x = day index)"), -->
<!--       x = "Day index (1 = first observed day for this market)", -->
<!--       y = "Probability", -->
<!--       color = "Model / Prompt" -->
<!--     ) + -->
<!--     theme_tud() + -->
<!--     theme( -->
<!--       legend.title = element_text(), -->
<!--       legend.key.height = unit(0.9, "lines"), -->
<!--       legend.text = element_text(size = 9) -->
<!--     ) -->
<!-- } -->
<!-- ``` -->

<!-- #### 5.4.1 plotting markets -->

<!-- ```{r save plots} -->
<!-- # --- 5.4.c Save one or save all (toggle) ------------------------------------- -->
<!-- # Set one (and only one) of these flags TRUE when you run the chunk. -->

<!-- SAVE_ONE <- TRUE     # save/preview a single market -->
<!-- SAVE_ALL <- FALSE    # loop and save every market (can be slow) -->

<!-- out_dir <- "market_plots_idx" -->
<!-- dir.create(out_dir, showWarnings = FALSE) -->

<!-- if (SAVE_ONE) { -->
<!--   one_id <- plot_data_idx$id[1]          # pick any id you want -->
<!--   g <- plot_market_idx(one_id) -->
<!--   print(g)                               # show in the viewer -->
<!--   # ggsave(file.path(out_dir, paste0(one_id, ".png")), g, width = 9, height = 5.5, dpi = 300) -->
<!-- } -->

<!-- if (SAVE_ALL) { -->
<!--   ids <- unique(plot_data_idx$id) -->
<!--   for (m in ids) { -->
<!--     g <- plot_market_idx(m) -->
<!--     ggsave(file.path(out_dir, paste0(m, ".png")), g, width = 9, height = 5.5, dpi = 300) -->
<!--   } -->
<!-- } -->
<!-- ``` -->

### 5.4.3 Conclution

LLMs can match early forecasts, but they fall behind as markets evolve.
The performance gap grows in higher-volume markets and in later stages, where real-time information aggregation becomes most active.
This suggests that markets learn, while LLMs remember.


# Chapter 6 - Forecast aggregation and the Iron Man model

In the previous chapters we have analysed and evaluated the predictive performance of each LLM in isolation and compared to the Polymarket.
In this chapter we change the scope and ask a different question: **can we create a combination of multiple forecasts to obtain probabilities that outperform the market itself?**


We treat the LLMs and Polymarket as separate experts that each produce their own probability forecasts. Based on these assessments we then create a number of aggregated forecasts, going simple to complex, starting with an elementary equal-weight avrage and then moving on to more complex regression based combination models.
For each aggregation we then evaluate various performance indicators, Brier scores, absolute errors at the market level, and test if the aggregated prediction improved on Polymarket's performance.
In case of even the best linear combination of LLMs and Polymarket fails to beat the market, that tells us that the information contained in the LLM forecasts does not add any predictive value beyond what is already reflected in the prices.

## 6.1 LLM-only aggregations



### 6.1.1 Equal-weight LLM average vs Polymarket

the first aggregation experiment is the simplest one, a simple **LLM crowd forecast** by taking the average of all available LLM forecasts for each market and stage (first, mid, last).
For a given market $i$ and stage $s$ with $K_{is}$ available LLM forecasts, the aggregated prediction is

$$
\hat p^{\text{LLM-avg}}_{is} = \frac{1}{K_{is}} \sum_{k=1}^{K_{is}} p_{isk}.
$$

After that we compare the stage specific LLM average to the Polymarket probabilities observed at the same stages.
For every market and stage we calculate Brier scores and absolute errors for both forecasts and use paired Wilcoxon signed-rank tests to assess how the LLM crowd performs compared to the market.

```{r llm_avg_vs_market_stages, message=FALSE, warning=FALSE}
# 6.2.1 Equal-weight LLM average vs Polymarket (first, mid, last stages)

suppressPackageStartupMessages({
  library(dplyr)
  library(kableExtra)
})

# Base data: stage snapshots for each model × prompt × market
mix_base <- temporal_vol %>%
  filter(stage %in% c("first", "mid", "last")) %>%
  # keep only rows with valid probabilities and resolution
  filter(is.finite(forecast), is.finite(yes_price), !is.na(resolution01))

# Construct market-level LLM average and corresponding market probability per stage
llm_avg_stage <- mix_base %>%
  group_by(id, stage) %>%
  summarise(
    question      = dplyr::first(question),
    p_llm_avg     = mean(forecast, na.rm = TRUE),
    p_market      = dplyr::first(yes_price),
    resolution01  = dplyr::first(resolution01),
    .groups       = "drop"
  ) %>%
  mutate(
    brier_llm_avg = (p_llm_avg   - resolution01)^2,
    brier_market  = (p_market    - resolution01)^2,
    abs_llm_avg   = abs(p_llm_avg - resolution01),
    abs_market    = abs(p_market  - resolution01),
    diff_brier    = brier_llm_avg - brier_market,
    diff_abs      = abs_llm_avg   - abs_market
  )

# Descriptive summary by stage
llm_avg_summary <- llm_avg_stage %>%
  group_by(stage) %>%
  summarise(
    n_markets         = dplyr::n_distinct(id),
    mean_brier_llm    = mean(brier_llm_avg),
    mean_brier_mkt    = mean(brier_market),
    median_brier_llm  = median(brier_llm_avg),
    median_brier_mkt  = median(brier_market),
    mean_abs_llm      = mean(abs_llm_avg),
    mean_abs_mkt      = mean(abs_market),
    median_abs_llm    = median(abs_llm_avg),
    median_abs_mkt    = median(abs_market),
    .groups = "drop"
  ) %>%
  arrange(factor(stage, levels = c("first","mid","last")))

kbl(
  llm_avg_summary,
  digits = 4,
  caption = "Summary of Brier scores and absolute errors for equal-weight LLM average vs Polymarket, by stage."
) %>%
  kable_styling(full_width = FALSE)

# Wilcoxon tests per stage on differences (equivalent to paired test)
wilcox_results_llm_avg <- llm_avg_stage %>%
  group_by(stage) %>%
  summarise(
    n_markets = n_distinct(id),

    # Brier differences
    median_diff_brier = median(diff_brier),
    mean_diff_brier   = mean(diff_brier),
    V_brier = tryCatch(
      wilcox.test(diff_brier, mu = 0, exact = FALSE)$statistic,
      error = function(e) NA
    ),
    p_brier = tryCatch(
      wilcox.test(diff_brier, mu = 0, exact = FALSE)$p.value,
      error = function(e) NA
    ),
    r_rosenthal_brier = tryCatch(
      rosenthal_r(wilcox.test(diff_brier, mu = 0, exact = FALSE)$p.value, n_markets),
      error = function(e) NA
    ),
    r_rank_biserial_brier = tryCatch(
      rank_biserial_signed(diff_brier, rep(0, length(diff_brier))),
      error = function(e) NA
    ),

    # Absolute error differences
    median_diff_abs = median(diff_abs),
    mean_diff_abs   = mean(diff_abs),
    V_abs = tryCatch(
      wilcox.test(diff_abs, mu = 0, exact = FALSE)$statistic,
      error = function(e) NA
    ),
    p_abs = tryCatch(
      wilcox.test(diff_abs, mu = 0, exact = FALSE)$p.value,
      error = function(e) NA
    ),
    r_rosenthal_abs = tryCatch(
      rosenthal_r(wilcox.test(diff_abs, mu = 0, exact = FALSE)$p.value, n_markets),
      error = function(e) NA
    ),
    r_rank_biserial_abs = tryCatch(
      rank_biserial_signed(diff_abs, rep(0, length(diff_abs))),
      error = function(e) NA
    ),

    .groups = "drop"
  ) %>%
  mutate(
    direction_brier = dplyr::case_when(
      median_diff_brier < 0 ~ "LLM average better",
      median_diff_brier > 0 ~ "Market better",
      TRUE                  ~ "No meaningful difference"
    ),
    direction_abs = dplyr::case_when(
      median_diff_abs < 0 ~ "LLM average better",
      median_diff_abs > 0 ~ "Market better",
      TRUE                ~ "No meaningful difference"
    )
  ) %>%
  arrange(factor(stage, levels = c("first","mid","last")))

wilcox_results_llm_avg_print <- wilcox_results_llm_avg %>%
  dplyr::select(
    -dplyr::ends_with("_abs"),      # drop ALL absolute-error columns
    -dplyr::starts_with("mean_"),   # drop mean differences
    -dplyr::starts_with("direction")
  )
# ---- Presentation layer: rename columns ----
col_labels <- c(
  stage                  = "Stage",
  n_markets              = "N markets",
  median_diff_brier      = "Median Δ Brier (LLM − Market)",
  V_brier                = "Wilcoxon V",
  p_brier                = "p-value",
  r_rosenthal_brier      = "Rosenthal r",
  r_rank_biserial_brier  = "Rank-biserial r"
)

rename_map <- stats::setNames(names(col_labels), col_labels)

wilcox_results_llm_avg_print <- wilcox_results_llm_avg_print %>%
  dplyr::rename(!!!rename_map)


kbl(
  wilcox_results_llm_avg_print,
  digits = 4,
  caption = "Wilcoxon signed-rank results comparing equal-weight LLM average to Polymarket by stage."
) %>%
  kable_styling(full_width = FALSE)
```

***Interpretation***

Looking at the results from from taking the average of the LLM predictions it is clear that they still under perform the Polymarket.
The LLM crowd's mean and median Brier scores are around **three times greater** than those of the market at every stage, and the absolute errors show a similar pattern.

The Wilcoxon test results confirm this pattern statistically.
For all stages the median difference in Brier score is positive, suggesting consistently higher errors for the LLM crowd.
All p-values are effectively 0, with effect sizes (Rosenthal’s $r$ = 0.68-0.77, rank-biserial = 0.79-0.95) suggesting a **large and consistent disadvantage** for the LLM crowd forecast.

Looking at these results it is clear that a simple average crowd aggregation of LLM probability does **not** extract any useful additional information.
It even suggests that averaging the LLMs boosts their bias relative to the market, rather than reducing it.

### 6.1.2 Performance-weighted LLM average vs Polymarket

The equal eight averaging used in 6.1.1 assumes that each model is equally useful and contributes equal amount of information.
This we know to be wrong form earlier analysis and therefore we want to test if using a performance based weight could improve the LLM crowd forecast.
To do so we create a weighted average where each model revives a weight negatively correlated to its historical Brier score.
Models with lower Brier scores revive a greater weight.

Weights are calculated from each model's **market-level mean Brier score at the last stage**, then normalized to sum to one.
These weights are then applied to the model-level predictions at all stages.
the outcomes will then be compared to those of the Polymarkert at each stage.

```{r llm_weighted_vs_market, message=FALSE, warning=FALSE}
suppressPackageStartupMessages({
  library(dplyr)
  library(kableExtra)
})

# -----------------------------
# 1) Compute model-level performance from LAST stage
# -----------------------------
model_perf <- temporal_vol %>%
  filter(stage == "last") %>%
  group_by(model) %>%
  summarise(
    mean_brier = mean(brier_llm, na.rm = TRUE),
    .groups = "drop"
  )

# Convert to weights: better models (lower Brier) get higher weight
model_weights <- model_perf %>%
  mutate(
    inv_brier = 1 / mean_brier,
    weight = inv_brier / sum(inv_brier)
  ) %>%
  select(model, weight)

# -----------------------------
# 2) Merge weights back into stage-level data
# -----------------------------
mix_weighted <- temporal_vol %>%
  filter(stage %in% c("first", "mid", "last")) %>%
  left_join(model_weights, by = "model") %>%
  # If any model has missing weight (shouldn't happen), drop
  filter(!is.na(weight))

# -----------------------------
# 3) Construct weighted LLM forecast per market × stage
# -----------------------------
llm_weighted_stage <- mix_weighted %>%
  group_by(id, stage) %>%
  summarise(
    question      = dplyr::first(question),
    p_llm_weighted = sum(weight * forecast) / sum(weight),
    p_market       = dplyr::first(yes_price),
    resolution01   = dplyr::first(resolution01),
    .groups        = "drop"
  ) %>%
  mutate(
    brier_llm_weighted = (p_llm_weighted - resolution01)^2,
    brier_market       = (p_market       - resolution01)^2,
    abs_llm_weighted   = abs(p_llm_weighted - resolution01),
    abs_market         = abs(p_market      - resolution01),
    diff_brier         = brier_llm_weighted - brier_market,
    diff_abs           = abs_llm_weighted   - abs_market
  )

# -----------------------------
# 4) Summary table
# -----------------------------
weighted_summary <- llm_weighted_stage %>%
  group_by(stage) %>%
  summarise(
    n_markets        = n_distinct(id),
    mean_brier_llm   = mean(brier_llm_weighted),
    mean_brier_mkt   = mean(brier_market),
    median_brier_llm = median(brier_llm_weighted),
    median_brier_mkt = median(brier_market),
    mean_abs_llm     = mean(abs_llm_weighted),
    mean_abs_mkt     = mean(abs_market),
    median_abs_llm   = median(abs_llm_weighted),
    median_abs_mkt   = median(abs_market),
    .groups = "drop"
  ) %>%
  arrange(factor(stage, levels = c("first","mid","last")))

kbl(
  weighted_summary,
  digits = 4,
  caption = "Summary of Brier scores and absolute errors for performance-weighted LLM average vs Polymarket, by stage."
) %>% kable_styling(full_width = FALSE)

# -----------------------------
# 5) Wilcoxon & effect sizes
# -----------------------------
wilcox_weighted <- llm_weighted_stage %>%
  group_by(stage) %>%
  summarise(
    n_markets = n_distinct(id),

    median_diff_brier = median(diff_brier),
    mean_diff_brier   = mean(diff_brier),
    V_brier = tryCatch(wilcox.test(diff_brier, mu = 0, exact = FALSE)$statistic, error = function(e) NA),
    p_brier = tryCatch(wilcox.test(diff_brier, mu = 0, exact = FALSE)$p.value, error = function(e) NA),
    r_rosenthal_brier = tryCatch(rosenthal_r(p_brier, n_markets), error = function(e) NA),
    r_rank_biserial_brier = tryCatch(rank_biserial_signed(diff_brier, rep(0,length(diff_brier))), error = function(e) NA),

    median_diff_abs = median(diff_abs),
    mean_diff_abs   = mean(diff_abs),
    V_abs = tryCatch(wilcox.test(diff_abs, mu = 0, exact = FALSE)$statistic, error = function(e) NA),
    p_abs = tryCatch(wilcox.test(diff_abs, mu = 0, exact = FALSE)$p.value, error = function(e) NA),
    r_rosenthal_abs = tryCatch(rosenthal_r(p_abs, n_markets), error = function(e) NA),
    r_rank_biserial_abs = tryCatch(rank_biserial_signed(diff_abs, rep(0,length(diff_abs))), error = function(e) NA),

    .groups = "drop"
  ) %>%
  mutate(
    direction_brier = case_when(
      median_diff_brier < 0 ~ "Weighted LLM better",
      median_diff_brier > 0 ~ "Market better",
      TRUE                  ~ "No meaningful difference"
    ),
    direction_abs = case_when(
      median_diff_abs < 0 ~ "Weighted LLM better",
      median_diff_abs > 0 ~ "Market better",
      TRUE                ~ "No meaningful difference"
    )
  ) %>%
  arrange(factor(stage, levels = c("first","mid","last")))

kbl(
  wilcox_weighted,
  digits = 4,
  caption = "Wilcoxon signed-rank results comparing performance-weighted LLM average to Polymarket by stage."
) %>% 
  kable_styling(full_width = FALSE)

```

***Interpretation***

Looking at the results form 6.1.2 it is clear that weighting LLMs relitive to their performance dose not improve their accuracy relative to Polymarket.
On all stages the performance weighted LLM crowd average produces higher Brier scores and absolute errors than those of the market.
Comparing the weighted averages to the equal weighted averages, revels that there is only a marginal change in their results.
This suggests that the diffrence in model quality is not large enough to meaningfully change the aggregated forecast.

The paired Wilcoxon test results are comparable to those of 6.1.1 and confirm these results statistically.

Overall, the performance weighting dose little to nothing in closing the performance gap between the LLM crowd and the market.

#### 6.2.3.a 50/50 hybrid: equal-weight LLM average combined with Polymarket

To test if the LLM forecasts contain any valuable information additional to what is already reflected in the market prices, we create a simple hybrid forecast, that avrages the equal weight LLM crowd prediction with that of the Polymarket.

$$
\hat p_{\text{hybrid}} = 0.5\, p_{\text{LLM-avg}} + 0.5\, p_{\text{market}}.
$$

This hybrid represents the most elementary form of combining the LLM Crowd and market information, following the simple to complex methodology used before.
We then evaluate the Brier score and absolute error of the hybrid forecast and compare it to the Polymarket using Wilcoxon signed-rank test at every stage.

```{r hybrid_5050, message=FALSE, warning=FALSE}
suppressPackageStartupMessages({
  library(dplyr)
  library(kableExtra)
})

# Recompute equal-weight LLM averages (same as 6.2.1)
llm_avg_stage <- temporal_vol %>%
  filter(stage %in% c("first","mid","last")) %>%
  filter(is.finite(forecast), is.finite(yes_price), !is.na(resolution01)) %>%
  group_by(id, stage) %>%
  summarise(
    question      = first(question),
    p_llm_avg     = mean(forecast, na.rm = TRUE),
    p_market      = first(yes_price),
    resolution01  = first(resolution01),
    .groups = "drop"
  )

# -------------------------
# 50/50 hybrid
# -------------------------
hybrid_5050 <- llm_avg_stage %>%
  mutate(
    p_hybrid = 0.5 * p_llm_avg + 0.5 * p_market,
    brier_hybrid = (p_hybrid - resolution01)^2,
    abs_hybrid   = abs(p_hybrid - resolution01),
    brier_market = (p_market - resolution01)^2,
    abs_market   = abs(p_market - resolution01),
    diff_brier   = brier_hybrid - brier_market,
    diff_abs     = abs_hybrid   - abs_market
  )

# Summary table
hybrid_summary <- hybrid_5050 %>%
  group_by(stage) %>%
  summarise(
    n_markets        = n_distinct(id),
    mean_brier_hybrid= mean(brier_hybrid),
    mean_brier_market= mean(brier_market),
    median_brier_hybrid = median(brier_hybrid),
    median_brier_market = median(brier_market),
    mean_abs_hybrid  = mean(abs_hybrid),
    mean_abs_market  = mean(abs_market),
    median_abs_hybrid= median(abs_hybrid),
    median_abs_market= median(abs_market),
    .groups = "drop"
  ) %>%
  arrange(factor(stage, levels = c("first","mid","last")))

kbl(
  hybrid_summary,
  digits = 4,
  caption = "Summary of Brier scores and absolute errors for 50/50 LLM–Polymarket hybrid vs Polymarket, by stage."
) %>%
  kable_styling(full_width = FALSE)

# -------------------------
# Wilcoxon tests per stage
# -------------------------
wilcox_hybrid_5050 <- hybrid_5050 %>%
  group_by(stage) %>%
  summarise(
    n_markets = n_distinct(id),
    median_diff_brier = median(diff_brier),
    mean_diff_brier   = mean(diff_brier),
    V_brier = tryCatch(wilcox.test(diff_brier, mu=0, exact=FALSE)$statistic, error=function(e) NA),
    p_brier = tryCatch(wilcox.test(diff_brier, mu=0, exact=FALSE)$p.value, error=function(e) NA),
    r_rosenthal_brier = tryCatch(rosenthal_r(p_brier, n_markets), error=function(e) NA),
    r_rank_biserial_brier = tryCatch(rank_biserial_signed(diff_brier, rep(0,length(diff_brier))), error=function(e) NA),

    median_diff_abs = median(diff_abs),
    mean_diff_abs   = mean(diff_abs),
    V_abs = tryCatch(wilcox.test(diff_abs, mu=0, exact=FALSE)$statistic, error=function(e) NA),
    p_abs = tryCatch(wilcox.test(diff_abs, mu=0, exact=FALSE)$p.value, error=function(e) NA),
    r_rosenthal_abs = tryCatch(rosenthal_r(p_abs, n_markets), error=function(e) NA),
    r_rank_biserial_abs = tryCatch(rank_biserial_signed(diff_abs, rep(0,length(diff_abs))), error=function(e) NA),

    direction_brier = case_when(
      median_diff_brier < 0 ~ "Hybrid better",
      median_diff_brier > 0 ~ "Market better",
      TRUE                  ~ "No meaningful difference"
    ),
    direction_abs = case_when(
      median_diff_abs < 0 ~ "Hybrid better",
      median_diff_abs > 0 ~ "Market better",
      TRUE                ~ "No meaningful difference"
    ),

    .groups = "drop"
  ) %>%
  arrange(factor(stage, levels=c("first","mid","last")))

kbl(
  wilcox_hybrid_5050,
  digits = 4,
  caption = "Wilcoxon tests comparing 50/50 hybrid to Polymarket, by stage."
) %>%
  kable_styling(full_width = FALSE)

```

***Interpretation***

Looking at the results of 6.2.3.a we can see that the 50/50 LLM crowd/Polymarket hybrid lowers the error compared to the LLM only forecast, although it dose **not** outperform the Polymarket at any stage.
Across all the market stages the hybrid's Brier score and absolute errors remain higher than those of the Polymarket alone.

The results of the Wilcoxon tests confirm this conclusion, all state-level differences are significantly positive (*p* = 0) with large effect sizes (*r* ≈ 0.68–0.83), indicating **higher error for the hybrid** in every case.

In conclusion, a simple equal-weight hybrid combination does not improve on Polymarket’s accuracy.

#### 6.2.3.b Equal-weight LLMs + Market

Now we test a simple combination where all the LLMs and the market receive equal weights.
This evaluates if adding market information to the LLM crowd improves accuracy.

```{r equal_weight_llm_market_full, message=FALSE, warning=FALSE}
suppressPackageStartupMessages({
  library(dplyr)
  library(kableExtra)
})

# 6.2.3.b Equal-weight LLMs + market (all forecasters get same weight)

# Build per-market × stage data
hybrid_equal_llm_market <- temporal_vol %>%
  filter(stage %in% c("first","mid","last")) %>%
  filter(is.finite(forecast), is.finite(yes_price), !is.na(resolution01)) %>%
  group_by(id, stage) %>%
  summarise(
    question      = dplyr::first(question),
    p_market      = dplyr::first(yes_price),
    n_llm         = dplyr::n(),                  # number of LLM forecasts
    p_llm_mean    = mean(forecast, na.rm = TRUE),
    resolution01  = dplyr::first(resolution01),
    .groups       = "drop"
  ) %>%
  mutate(
    # Equal weight for each LLM and the market: (sum LLMs + market) / (K + 1)
    p_hybrid      = (p_llm_mean * n_llm + p_market) / (n_llm + 1),
    brier_hybrid  = (p_hybrid  - resolution01)^2,
    brier_market  = (p_market  - resolution01)^2,
    abs_hybrid    = abs(p_hybrid  - resolution01),
    abs_market    = abs(p_market  - resolution01),
    diff_brier    = brier_hybrid - brier_market,
    diff_abs      = abs_hybrid   - abs_market
  )

# -----------------------------
# Summary table
# -----------------------------
summary_equal_hybrid <- hybrid_equal_llm_market %>%
  group_by(stage) %>%
  summarise(
    n_markets           = dplyr::n_distinct(id),
    mean_brier_hybrid   = mean(brier_hybrid),
    mean_brier_market   = mean(brier_market),
    median_brier_hybrid = median(brier_hybrid),
    median_brier_market = median(brier_market),
    mean_abs_hybrid     = mean(abs_hybrid),
    mean_abs_market     = mean(abs_market),
    median_abs_hybrid   = median(abs_hybrid),
    median_abs_market   = median(abs_market),
    .groups = "drop"
  ) %>%
  arrange(factor(stage, levels = c("first","mid","last")))

kableExtra::kbl(
  summary_equal_hybrid,
  digits = 4,
  caption = "Summary of Brier scores and absolute errors for equal-weight LLM+market hybrid vs Polymarket, by stage."
) %>%
  kableExtra::kable_styling(full_width = FALSE)

# -----------------------------
# Wilcoxon tests + effect sizes
# -----------------------------
wilcox_equal_hybrid <- hybrid_equal_llm_market %>%
  group_by(stage) %>%
  summarise(
    n_markets        = dplyr::n_distinct(id),

    # Brier
    median_diff_brier = median(diff_brier),
    mean_diff_brier   = mean(diff_brier),
    V_brier           = tryCatch(
                          wilcox.test(diff_brier, mu = 0, exact = FALSE)$statistic,
                          error = function(e) NA
                        ),
    p_brier           = tryCatch(
                          wilcox.test(diff_brier, mu = 0, exact = FALSE)$p.value,
                          error = function(e) NA
                        ),
    r_rosenthal_brier = tryCatch(
                          rosenthal_r(
                            wilcox.test(diff_brier, mu = 0, exact = FALSE)$p.value,
                            n_markets
                          ),
                          error = function(e) NA
                        ),
    r_rank_biserial_brier = tryCatch(
                              rank_biserial_signed(diff_brier, rep(0, length(diff_brier))),
                              error = function(e) NA
                            ),

    # Absolute error
    median_diff_abs   = median(diff_abs),
    mean_diff_abs     = mean(diff_abs),
    V_abs             = tryCatch(
                          wilcox.test(diff_abs, mu = 0, exact = FALSE)$statistic,
                          error = function(e) NA
                        ),
    p_abs             = tryCatch(
                          wilcox.test(diff_abs, mu = 0, exact = FALSE)$p.value,
                          error = function(e) NA
                        ),
    r_rosenthal_abs   = tryCatch(
                          rosenthal_r(
                            wilcox.test(diff_abs, mu = 0, exact = FALSE)$p.value,
                            n_markets
                          ),
                          error = function(e) NA
                        ),
    r_rank_biserial_abs = tryCatch(
                            rank_biserial_signed(diff_abs, rep(0, length(diff_abs))),
                            error = function(e) NA
                          ),
    .groups = "drop"
  ) %>%
  mutate(
    direction_brier = dplyr::case_when(
      median_diff_brier < 0 ~ "Hybrid better",
      median_diff_brier > 0 ~ "Market better",
      TRUE                  ~ "No meaningful difference"
    ),
    direction_abs = dplyr::case_when(
      median_diff_abs < 0 ~ "Hybrid better",
      median_diff_abs > 0 ~ "Market better",
      TRUE                ~ "No meaningful difference"
    )
  ) %>%
  arrange(factor(stage, levels = c("first","mid","last")))

kableExtra::kbl(
  wilcox_equal_hybrid,
  digits = 4,
  caption = "Wilcoxon signed-rank results for equal-weight LLM+market hybrid vs Polymarket, by stage."
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

***Interpretation***

Adding the Polymarket to the LLM crowd improves the performance compared to the pure LLM crowd, even though it is still less accurate than the stand alone Polymarket predictions.

Across all stages, the hybrid's Brier scores and absolute errors are higher than those of the Polymarket.
The wilcoxon test results confirm this pattern, median differences are positive with significant p-values.
This point towards a systematically higher error for the hybrid than stand alone Polymarket.

To conclude, the assembly dose not create forecasts that can rival those of the Polymarket alone.

#### 6.2.3.c Summary table for results

```{r aggregation copmparison table, message=FALSE, warning=FALSE}
# -----------------------------
# 6.2.x Combined Wilcoxon table (Brier only)
# -----------------------------

wilcox_eq_llm_avg_core <- wilcox_results_llm_avg %>%
  dplyr::select(
    stage,
    n_markets,
    median_diff_brier,
    V_brier,
    p_brier,
    r_rosenthal_brier,
    r_rank_biserial_brier
  ) %>%
  dplyr::mutate(method = "Equal-weight LLM avg") %>%
  dplyr::select(method, dplyr::everything())

wilcox_hybrid_5050_core <- wilcox_hybrid_5050 %>%
  dplyr::select(
    stage,
    n_markets,
    median_diff_brier,
    V_brier,
    p_brier,
    r_rosenthal_brier,
    r_rank_biserial_brier
  ) %>%
  dplyr::mutate(method = "50/50 hybrid") %>%
  dplyr::select(method, dplyr::everything())

wilcox_equal_hybrid_core <- wilcox_equal_hybrid %>%
  dplyr::select(
    stage,
    n_markets,
    median_diff_brier,
    V_brier,
    p_brier,
    r_rosenthal_brier,
    r_rank_biserial_brier
  ) %>%
  dplyr::mutate(method = "Equal-weight LLM+market") %>%
  dplyr::select(method, dplyr::everything())

wilcox_combined <- dplyr::bind_rows(
  wilcox_eq_llm_avg_core,
  wilcox_hybrid_5050_core,
  wilcox_equal_hybrid_core
) %>%
  dplyr::mutate(
    stage = factor(stage, levels = c("first", "mid", "last")),
    method = factor(method, levels = c("Equal-weight LLM avg", "50/50 hybrid", "Equal-weight LLM+market"))
  ) %>%
  dplyr::arrange(method, stage)

# ---- Presentation layer: rename columns ----
col_labels <- c(
  method                = "Method",
  stage                 = "Stage",
  n_markets             = "N markets",
  median_diff_brier     = "Median Δ Brier",
  V_brier               = "Wilcoxon V",
  p_brier               = "p-value",
  r_rosenthal_brier     = "Rosenthal r",
  r_rank_biserial_brier = "Rank-biserial r"
)

rename_map <- stats::setNames(names(col_labels), col_labels)

wilcox_combined_print <- wilcox_combined %>%
  dplyr::rename(!!!rename_map)

kableExtra::kbl(
  wilcox_combined_print,
  digits = 4,
  caption = "Wilcoxon signed-rank results (Brier) across aggregation methods and stages. Δ Brier = Method - Market."
) %>%
  kableExtra::kable_styling(full_width = FALSE) %>%
  kableExtra::group_rows(
    "Equal-weight LLM avg",
    start_row = 1,
    end_row = 3,
    bold = TRUE
  ) %>%
  kableExtra::group_rows(
    "50/50 hybrid",
    start_row = 4,
    end_row = 6,
    bold = TRUE
  ) %>%
  kableExtra::group_rows(
    "Equal-weight LLM + market",
    start_row = 7,
    end_row = 9,
    bold = TRUE
  )
```

### 6.2.4 Cherry-picked hybrid: best LLM + market

Now we test if cherry picking the **best** LLM with the Polymarket can improve forecasting accuracy.
We do that by identifying the single best performing model and prompt combination for each stage based on their mean Brier scores.
We then create a cherry-picked hybrid forecast.

$$
\hat p_{\text{hybrid}} = w\,p_{\text{LLM}} + (1-w)\,p_{\text{market}},
$$

where $w \in [0,1]$ is a weight on the LLM prediction.
We iterate over weights from 0 to 1 in 1% increments, we keep track of the weight that minimizes the mean Brier score for each stage.
Finally compare the optimal-weight hybrid to Polymarket using paired Wilcoxon signed-rank tests.

After all this, we test a cherry-picked hybrid that uses only the single best LLM model and combines it with Polymarket using a costume weight.
We evaluate weights from 0 to 1 in 5% increments to see whether any combination improves on the market’s Brier score or absolute error.
This identifies whether the best-performing LLM adds value to the market-based forecast.

```{r cherry_pick_best_model_prompt, message=FALSE, warning=FALSE}
# Identify best-performing model+prompt for each stage
best_llm_stage <- temporal_vol %>%
  group_by(model, prompt_type, stage) %>%
  summarise(
    mean_brier = mean(brier_llm, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  group_by(stage) %>%
  slice_min(mean_brier, n = 1) %>%
  ungroup()

best_llm_stage
```

```{r cherry_pick_weight_sweep, message=FALSE, warning=FALSE}
weights <- seq(0, 1, by = 0.05)

# Prepare dataset containing: id, stage, best LLM prob, market prob, resolution
hybrid_base <- temporal_vol %>%
  inner_join(best_llm_stage, by = c("model","prompt_type","stage")) %>%
  group_by(id, stage) %>%
  summarise(
    p_llm    = first(forecast),
    p_market = first(yes_price),
    resolution01 = first(resolution01),
    .groups = "drop"
  )

# Expand over all weights
hybrid_results <- expand.grid(weight = weights, stage = unique(hybrid_base$stage)) %>%
  left_join(hybrid_base, by = "stage") %>%
  mutate(
    p_hybrid = weight * p_llm + (1 - weight) * p_market,
    brier_hybrid = (p_hybrid - resolution01)^2,
    abs_hybrid   = abs(p_hybrid - resolution01),
    brier_market = (p_market - resolution01)^2,
    abs_market   = abs(p_market - resolution01),
    diff_brier = brier_hybrid - brier_market,
    diff_abs   = abs_hybrid - abs_market
  )

```

```{r cherry_pick_optimal_weights, message=FALSE, warning=FALSE}
optimal_weights <- hybrid_results %>%
  group_by(stage, weight) %>%
  summarise(
    mean_brier_hybrid = mean(brier_hybrid),
    mean_abs_hybrid   = mean(abs_hybrid),
    .groups = "drop"
  ) %>%
  group_by(stage) %>%
  slice_min(mean_brier_hybrid, n = 1)

optimal_weights

```

```{r cherry_pick_wilcox_optimal, message=FALSE, warning=FALSE}
# Filter optimal
hybrid_optimal <- hybrid_results %>%
  inner_join(optimal_weights %>% select(stage, weight),
             by = c("stage","weight"))

wilcox_cherry <- hybrid_optimal %>%
  group_by(stage) %>%
  summarise(
    n_markets = n_distinct(id),
    median_diff_brier = median(diff_brier),
    V_brier = wilcox.test(diff_brier, mu = 0, exact = FALSE)$statistic,
    p_brier = wilcox.test(diff_brier, mu = 0, exact = FALSE)$p.value,
    median_diff_abs = median(diff_abs),
    V_abs = wilcox.test(diff_abs, mu = 0, exact = FALSE)$statistic,
    p_abs = wilcox.test(diff_abs, mu = 0, exact = FALSE)$p.value,
    direction_brier = ifelse(median_diff_brier > 0, "Market better", "Hybrid better"),
    direction_abs   = ifelse(median_diff_abs > 0, "Market better", "Hybrid better"),
    .groups = "drop"
  )

kbl(wilcox_cherry,
    caption = "Wilcoxon test comparing optimal-weight hybrid vs Polymarket.",
    digits = 4) %>%
  kable_styling(full_width = FALSE)

```

***Interpretation***

Looking at the results from 6.2.4, we can see that across all stages, the cherry picked hybrid fails to deliver significant improvements over the Polymarket alone.
After calculating the optimal weights for the very best model for each stage, we found out that the optimal weight for both the first and mid stages are **no weight on the LLM**, this implies that any contribution from the best model-prompt combination would worsen the results.
For the last stage, a very small LLM weight (0.05) can marginally lower the Brier score.
Even though the Brier score is marginally lower the Wilcoxon test results still favor stand alone Polymarket predictions.

To conclude, even under extremely favorable conditions, where we cherry-pick the best LLMs and optimize their weights, the hybrid combination still fails to outperform the Polymarket predictions.
This point to the concoction that the Polymarket is the best forecaster within this dataset and that the LLM forecasts add no meaningful predictive value to it.

## 6.3 Iron Man Model: Optimal Forecast Combination via Logistic Regression

### 6.3.1 Motivation

The previous sections explored a variety of hybrid constructions—equal-weight averages, performance-weighted ensembles, and cherry-picked combinations.\
It is clear that none of them consistently outperform Polymarket.

This motivates a final, more powerful approach: a **full information forecast combination model** that uses\
*all available prediction signals simultaneously* and learns **optimal weights** directly from the data.

Following the recommendation of my supervisor, this model treats each LLM prediction and the Polymarket price as input variables and learns how to best combine them to predict the final outcome.
Because the outcome is binary (0/1), the natural specification is a **logistic regression**:

$$
\Pr(Y = 1 \mid X) = \text{logit}^{-1}\!\left(
\beta_0 + \beta_{\text{mkt}} p_{\text{mkt}} + 
\sum_{k=1}^K \beta_k p_{k}
\right),
$$

where\
- $p_{\text{mkt}}$ is the Polymarket price,\
- $p_k$ are the LLM forecasts from model–prompt pairs,\
- $Y$ is the realized outcome,\
- and $\beta$ are estimated combination weights.

This model constitutes the “**Iron Man**” version of the hybrid: a theoretically grounded, statistically optimal combination of all forecast inputs.\

### 6.3.2 Data Construction for the Iron Man Model

To estimate the optimal combination, I construct a dataset where each row corresponds to one\
**market × stage (first, mid, last)** observation.\
For each such observation, I include:

-   the realized outcome (`resolution01`),\
-   the Polymarket probability (`yes_price`),\
-   and forecast probabilities for all model–prompt pairs.

This dataset forms the input for stage-specific logistic regressions.


```{r add volume to ironman}
# 1) Volume lookup from data_vol
# Make sure the raw volume column is numeric.
# Replace 'volume_num' with the actual name in data_vol if different.
data_vol_clean <- data_vol %>%
  mutate(volume_num = as.numeric(volume_num))

# Collapse to one volume per market id (e.g., final or total volume)
volume_lookup <- data_vol_clean %>%
  group_by(id) %>%
  summarise(
    volume_total = max(volume_num, na.rm = TRUE),  # or sum(), mean() – pick what makes sense
    .groups = "drop"
  )

# 2) Define the final analysis sample (markets × stage)
valid_markets <- data_brier_valid %>%
  distinct(id)


```


```{r ironman-data}
valid_ids <- data_brier_valid %>%
  dplyr::distinct(id)

iron_data <- temporal_vol %>%
  dplyr::semi_join(valid_ids, by = "id") %>%
  group_by(id, stage) %>%
  summarise(
    y          = first(resolution01),
    p_market   = first(yes_price),
    p_no     = first(no_price),    # <- add NO price here


    # LLM forecasts (add all LLM–prompt combinations)
    google_12b_zs  = mean(forecast[model == "google_gemma_3_12b_it" & prompt_type == "Zero-shot"], na.rm = TRUE),
    google_12b_cot = mean(forecast[model == "google_gemma_3_12b_it" & prompt_type == "Chain-of-Thought"], na.rm = TRUE),

    deepseek_r1_zs  = mean(forecast[model == "deepseek_deepseek_r1_free" & prompt_type == "Zero-shot"], na.rm = TRUE),
    deepseek_r1_cot = mean(forecast[model == "deepseek_deepseek_r1_free" & prompt_type == "Chain-of-Thought"], na.rm = TRUE),

# --- OpenAI models ---
    openai_35_zs  = mean(
      forecast[model == "openai_gpt_3_5_turbo_0613" &
prompt_type == "Zero-shot"],
      na.rm = TRUE),
    openai_35_cot = mean(
      forecast[model == "openai_gpt_3_5_turbo_0613" &
prompt_type == "Chain-of-Thought"],
      na.rm = TRUE),

    openai_4o_zs  = mean(
      forecast[model == "openai_gpt_4o_mini" &
prompt_type == "Zero-shot"],
      na.rm = TRUE),
    openai_4o_cot = mean(
      forecast[model == "openai_gpt_4o_mini" &
prompt_type == "Chain-of-Thought"],
      na.rm = TRUE),

    google_27b_zs  = mean(
      forecast[model == "google_gemma_3_27b_it_free" &
               prompt_type == "Zero-shot"],
      na.rm = TRUE
    ),
    google_27b_cot = mean(
      forecast[model == "google_gemma_3_27b_it_free" &
               prompt_type == "Chain-of-Thought"],
      na.rm = TRUE
    ),

    # --- DeepSeek models ---
    deepseek_chat_zs  = mean(
      forecast[model == "deepseek_deepseek_chat_v3_0324_free" &
               prompt_type == "Zero-shot"],
      na.rm = TRUE
    ),
    deepseek_chat_cot = mean(
      forecast[model == "deepseek_deepseek_chat_v3_0324_free" &
               prompt_type == "Chain-of-Thought"],
      na.rm = TRUE
    ),

    .groups = "drop"
  ) %>%
  # Attach total volume from data_vol
  left_join(volume_lookup, by = "id")



```





### 6.3.3.1 Correlation Between Market and LLM Forecasts



```{r ironman-correlations}
library(dplyr)
library(kableExtra)

# Select the relevant probability columns
corr_vars <- c(
  "p_market",
  "openai_35_zs",  "openai_35_cot",
  "openai_4o_zs",  "openai_4o_cot",
  "google_12b_zs", "google_12b_cot",
  "google_27b_zs", "google_27b_cot",
  "deepseek_chat_zs", "deepseek_chat_cot",
  "deepseek_r1_zs",   "deepseek_r1_cot"
)

# Compute and print one correlation matrix per stage
for (s in unique(iron_data$stage)) {

  cat("### Correlation matrix for stage:", s, "\n\n")

  # Subset only this stage & drop rows containing NA (if any)
  stage_data <- iron_data %>%
    filter(stage == s) %>%
    select(all_of(corr_vars)) %>%
    na.omit()

  # Compute correlation
  corr_mat <- cor(stage_data, use = "pairwise.complete.obs")

  # Print nicely
  print(
    kableExtra::kbl(
      round(corr_mat, 3),
      caption = paste0("Correlation matrix of market and LLM forecasts (", s, " stage)")
    ) %>%
      kableExtra::kable_styling(full_width = FALSE)
  )

  cat("\n\n")
}
```

***Interpretation***

Looking at the correlation matrix for the fist stage, the market and LLM forecasts are moderetly to highly correlated (0.56-0.73).
The LLMs tend to be more correlated with one another than with the market.
The correlation between the market and the LLMs indicates that different predictors still show some variation and disagree on some market predictions.

At the mid stage, the correlations modestly increase (0.58-0.77), showing that the LLMs and the market are slowly getting closer to one another with their predictions.

At the last stage, the correlation relationship changes slightly.
The more powerful DeepSeek and Gemma models still show a strong correlation with the market, while the weaker models of OpenAI showed weaker correlations.

### 6.3.3.2 Mean and standar deviation table

```{r ironman-descriptives}
library(dplyr)
library(kableExtra)

desc_vars <- c(
  "p_market",
  "openai_35_zs",  "openai_35_cot",
  "openai_4o_zs",  "openai_4o_cot",
  "google_12b_zs", "google_12b_cot",
  "google_27b_zs", "google_27b_cot",
  "deepseek_chat_zs", "deepseek_chat_cot",
  "deepseek_r1_zs",   "deepseek_r1_cot"
)

iron_desc <- iron_data %>%
  group_by(stage) %>%
  summarise(
    across(
      all_of(desc_vars),
      list(
        mean = ~mean(.x, na.rm = TRUE),
        sd   = ~sd(.x, na.rm = TRUE)
      ),
      .names = "{.col}_{.fn}"
    )
  )

iron_desc_rounded <- iron_desc %>%
  mutate(across(where(is.numeric), ~round(.x, 4)))

kableExtra::kbl(
  iron_desc_rounded,
  caption = "Means and standard deviations of Polymarket and LLM forecasts by stage"
) %>%
  kableExtra::kable_styling(full_width = FALSE)

```

\***Interpretation**

The standard deviation shows how the spread of predictions changes over time.
Early forecasts form both Polymarket and the LLMs show a moderate variability, reflecting an initial uncertainty about outcomes.
As time continues (mid and late stage) the variation in the Polymarket predictions increases, this is consistent with probabilities polarizing towards either 0 or 1 as the markets approach resolution.
Most LLM models are either stable or show a slight increase in variation across stages, this suggests that their forecasts also differentiate more across markets as additional information comes to light, though less than the Polymarket.

When we look at this together with the correlation patterns in chapter 6.3.3.1, there is an indication that both the Polymarket and the LLMs adjust their probability assessment over the lifetime of the markets.
They do this in a way that keeps different predictors strongly aligned with one another.

#### 6.3.4 Estimating Stage-Specific Logistic Regressions

For each stage (first, mid, last), I estimate a logistic regression where\
the outcome is the realized resolution (`y`) and all forecast signals enter as predictors:

```{r ironman-regress}
# 1. List all predictor columns (market + all LLMs)
iron_predictors <- c(
  "p_market",
  # OpenAI
  "openai_35_zs",  "openai_35_cot",
  "openai_4o_zs",  "openai_4o_cot",
  # Google
  "google_12b_zs", "google_12b_cot",
  "google_27b_zs", "google_27b_cot",
  # DeepSeek
  "deepseek_chat_zs", "deepseek_chat_cot",
  "deepseek_r1_zs",   "deepseek_r1_cot"
)

# 2. Build the logistic regression formula dynamically
iron_formula <- as.formula(
  paste("y ~", paste(iron_predictors, collapse = " + "))
)

# 3. Fit separate models for each stage
iron_models <- iron_data %>%
  group_by(stage) %>%
  do({
    m <- glm(
      formula = iron_formula,
      data    = .,
      family  = binomial(link = "logit")
    )
    tibble(model = list(m))
  })

# print(iron_models)  # optional
```

### 6.3.5 Iron Man coefficients

```{r ironman-coefs-by-stage}
suppressPackageStartupMessages({
  library(dplyr)
  library(tidyr)
  library(purrr)
  library(broom)
  library(kableExtra)
  library(webshot2)
})

iron_coef_table <- iron_models %>%
  mutate(tidy = purrr::map(model, broom::tidy)) %>%
  tidyr::unnest(tidy) %>%
  select(stage, term, estimate, std.error, statistic, p.value) %>%
  dplyr::ungroup()

col_labels <- c(
  term      = "Term",
  estimate  = "Estimate",
  std.error = "Std. Error",
  statistic = "z value",
  p.value   = "p-value"
)
rename_map <- stats::setNames(names(col_labels), col_labels)

a4_w <- 1123
a4_h <- 794

for (s in unique(iron_coef_table$stage)) {

  tab <- iron_coef_table %>%
    filter(stage == s) %>%
    select(term, estimate, std.error, statistic, p.value) %>%
    rename(!!!rename_map)

  kbl_obj <- kableExtra::kbl(
    tab,
    digits = 4,
    caption = paste0("Iron Man logistic regression coefficients (", s, " stage)")
  ) %>%
    kableExtra::kable_styling(full_width = TRUE, position = "left", font_size = 12)

  html_file <- paste0("ironman_coefs_", s, ".html")
  pdf_file  <- paste0("ironman_coefs_", s, "_A4.pdf")

  kableExtra::save_kable(kbl_obj, file = html_file)
  webshot2::webshot(
    url = html_file,
    file = pdf_file,
    vwidth = a4_w,
    vheight = a4_h,
    zoom = 2
  )
}
```


***Interpretation***

Looking at the coefficient table it may look strange at first. Many estimates are large, the standard errors are even larger, and all of the p-values are close to 1. The signs are inconsistent between stages and there is no clear pattern that can be found suggesting that any one model consistently receives more weight than the next one.

This may seem surprising at first, although when we look at the structure of the predictors as shown in the correlation matrices, all LLM forecasts are highly correlated with one another and also with the Polymarket prices. When looking at the summary statistics from section 6.3.3 we see that they are on very similar scales. Due to the fact that all the predictors move almost identically, the logistic regression has almost no independent variation to work with. This causes multicollinearity and quasi-separation**(ADD TO THEORATICAL BACKROUND)**, causing the standard errors to increase and makes the coefficient estimates unstable and uninterpreted.

To conclude, the regression can not separate the contribution of any single LLM forecast or even the market to a meaningful degree. All the predictors are made up of essentially the same information. This gives strong evidence for evaluating the Iron Man model on its over all predictive performance, rather than through evaluation of the individual regression coefficients, as they do not carry reliable economic meaning.

### 6.3.6 Generating Iron Man Predictions

Using the fitted models, we generate predicted probabilities for each market × stage:

```{r ironman-predict}
iron_predictions <- iron_data %>%
  left_join(iron_models, by = "stage") %>%
  mutate(
    p_iron = map2_dbl(model, row_number(), ~{
      predict(.x, newdata = iron_data[.y,], type = "response")
    })
  )

```

### 6.3.8 Comparing Iron Man to Polymarket

To evaluate whether the Iron Man model improves accuracy,\
We compute Brier scores and run paired Wilcoxon signed-rank tests.

```{r ironman-eval}
iron_comparison <- iron_predictions %>%
  mutate(
    brier_market = (p_market - y)^2,
    brier_iron   = (p_iron - y)^2,
    diff_brier   = brier_iron - brier_market
  ) %>%
  group_by(stage) %>%
  summarise(
    n_markets   = n(),
    median_diff = median(diff_brier),
    mean_diff   = mean(diff_brier),
    p_value     = wilcox.test(diff_brier, mu = 0, exact = FALSE)$p.value,
    .groups     = "drop"
  ) %>%
  mutate(
    stage = factor(stage, levels = c("first", "mid", "last"))
  ) %>%
  arrange(stage)

# ---- Presentation layer: rename columns ----
col_labels <- c(
  stage       = "Stage",
  n_markets   = "N markets",
  median_diff = "Median Δ Brier",
  mean_diff   = "Mean Δ Brier",
  p_value     = "p-value"
)

rename_map <- stats::setNames(names(col_labels), col_labels)

iron_comparison_print <- iron_comparison %>%
  dplyr::rename(!!!rename_map)

kableExtra::kbl(
  iron_comparison_print,
  digits = 4,
  caption = "Iron Man vs Polymarket: Brier score differences and Wilcoxon tests. Δ Brier = Iron − Market."
) %>% 
  kableExtra::kable_styling(full_width = FALSE)
#buy and hold strat, 
```

***Interpretation***


The Iron Man model combines all the LLM forecasts and the Polymarket prices(forecast) using a logistic regression.
The model is equipped with an unfair advantage: it is estimated on the full data set of realized outcomes giving it access to information that would not be available to any real-time forecaster in practice.

The performance gain of the Iron Man compared to the Polymarket stand alone is negligibly small.
Looking at the Brier scores, they are positive in the first two stages, indicating that the Iron Man has a slightly better accuracy than that of the Polymarket.
On the late stage it has has the same score as that of the Polymarket alone, indicating that it has no advantage over the market.
The improvements using the Iron man are economically diminishing (magnitude of -(0.0005 - 0.0016), although the Wilcoxon test deems them statistically significant.

To conclude, although a carefully and optimally fitted combination model can marginally outperform the market in early and mid stages, these gains are so small that they are unlikely to survive in any realistic trading environment once transaction costs, timing frictions, and model updating are accounted for.
Polymarket prices therefore remain effectively as good as, and in practical terms even better than, any weighted combination of the available LLM forecasts.

### 6.3.9 Iron Man conclution

Looking at all the results from the Iron Man model, they paint a clear picture. Even when the model is given every advantage, in the best possible environment, with full information, access to all forecasts and the ability to fit weights on realized outcomes, the Iron Man model delivers only marginal improvements over the market, and that is only in the first and mid stages. By the time that the markets approach a resolution, the model isn't able to provide any measurable benefits at all.

This leads us to the conclusion that due to the fact that this Iron Man set up is optimized for these specific markets, and has the previously mentioned benefits, that no real time forecaster would ever have access to, and it is still only able to provide marginal predictive performance increase. Any real-time or out of sample method will perform strictly worse than our Iron Man model. The conclusion therefore is that the Polymarket already encapsulates almost all of the useful data available for these events, and that that any information provided by the army of LLMs is effectively redundant and adds little to no additional value for the predictability of the markets.



# Chapter 7 Trading simulation

## 7.1 From Accuracy to Tradeable Value

The results from chapter 6 show that the Iron Man model is able to marginally improve the Polymarket prediction accuracy in the first and middle stages of the markets in terms of Brier scores. The next and final step in the data analysis is to see if this slight edge can translate into meaningful economical gains.

In a prediction market environment, a model is of special interest if it can be turned into meaningful economical gains, a marginal improvement in forecasting accuracy is irrelevant if it doesn't survive in the actual market. This chapter shifts the spotlight from statistical performance and analysis to a more economically focused trading value. Given the Iron Man probabilities, could a trader have systematically earned money by taking positions against Polymarket pricing. Once again, we take a simple to complex approach. In the fist test the set up is deliberately generous to the Iron Man model. It assumes full information, no frictions, and perfect ability to execute trades. If the model can not make greater profit under these ideal conditions, then it will not make profit under any conditions. In that case, there is no reason to test more complex strategies, and this effectively concludes the empirical analysis of this thesis.

## 7.2 Trading Setup and Iron Man Trading Strategy This whole text has to be changed, NO LONGER VALID!

The statistical analysis in Chapter 6 showed that the Iron Man model achieves a small improvement over Polymarket’s accuracy in the early and mid stages. The central question now is whether this advantage has any economic value. A forecasting model only matters in a prediction market if its probability estimates can be converted into profits.

To evaluate this, I consider a simple, idealised trading environment based on Polymarket yes/no contracts. These contracts pay 1 if the event resolves as “yes” and 0 otherwise. At each stage (first, mid, last), the market quotes a price \(p_{\text{mkt}}\), while the Iron Man model provides its own probability estimate \(p_{\text{iron}}\).



### Two strategies

I evaluate two progressively more demanding trading strategies.

---

### **1. Long-only buy-and-hold strategy**

This is the simplest possible test. At a given stage, the strategy buys exactly one yes-share if and only if the Iron Man model assigns a higher probability to the event than the market does:

\[
p_{\text{iron}} > p_{\text{mkt}} \quad \Rightarrow \quad \text{buy one yes-share}.
\]

If Iron Man does not detect underpricing, no trade is made.  
All positions are held to resolution.

This setup removes strategic complexity and isolates the fundamental question:  
**Does Iron Man identify underpriced contracts well enough to produce positive expected returns?**

---

### **2. Long/short sign-based strategy**

The second strategy takes the opposite side of the market whenever Iron Man disagrees:

- If \(p_{\text{iron}} > p_{\text{mkt}}\): take a long position in “yes”  
- If \(p_{\text{iron}} < p_{\text{mkt}}\): take a short position in “yes”  
- Hold until resolution

This effectively implements a pure model-vs-market confrontation: whenever Iron Man’s beliefs diverge from the market, it acts on that difference.


## 7.3 Dataset overview

### 7.3.1 Ironman sanity check GPT text

“The final Iron Man dataset contains 159 observations (53 markets × 3 stages), with Iron Man probabilities and Polymarket YES/NO prices defined for almost all cases; the few missing NO prices are handled as described below.”

```{r ironman-sanity-checks}
iron_data %>%
  count(stage, name = "n_rows")

iron_data %>%
  summarise(
    n_rows       = n(),
    n_markets    = n_distinct(id),
    n_stage_combos = n_distinct(paste(id, stage))
  )

iron_predictions %>%
  summarise(
    n_rows         = n(),
    n_missing_p_iron  = sum(is.na(p_iron)),
    n_missing_p_market = sum(is.na(p_market)),
    n_missing_p_no     = sum(is.na(p_no))
  )
```

### 7.3.2 Handel missing values

As there are a few markets with a missing value for No_price that have to be taken care of. We identify the missing values and simulate them with a simple "approximation", 1-yes_price = no_price

```{r ironman-handle-p-no, message=FALSE}
iron_predictions_clean <- iron_predictions %>%
  mutate(
    # Flag original missing NO prices
    p_no_missing_raw = is.na(p_no),
    # Impute p_no when it is missing but p_market is available
    p_no = if_else(
      is.na(p_no) & !is.na(p_market),
      1 - p_market,
      p_no
    )
  )

iron_predictions_clean %>%
  summarise(
    n_rows           = n(),
    n_p_no_missing   = sum(p_no_missing_raw),
    pct_p_no_missing = mean(p_no_missing_raw)
  )
```
***Interpretation***
Looking at the results from 7.3.1 we can see that there are no longer any missing values for p_no, meaning that the analysis can continue.

##7.4 Simmulation execution

### 7.4.1 Buy-and-Hold strategy

Having constructed the Iron Man probabilities for each market and stage, the next step is to evaluate whether these forecasts support a profitable trading strategy.

We begin with the simplest possible implementation: a 1-share buy-and-hold strategy. For each market × stage observation, the trader either buys one YES contract or one NO contract depending on whether Iron Man assigns a higher probability than the Polymarket price.

This approach deliberately strips away all dynamic rebalancing or position sizing complexity. The goal here is not to simulate professional trading but to establish a clean baseline: if Iron Man cannot generate consistent profits under these frictionless, single-share conditions, it is unlikely to perform better under more realistic constraints.

The tables below summaries the stage-level and overall profit-and-loss (PnL) outcomes for this buy-and-hold strategy.


```{r ironman-longonly-sim, message=FALSE, warning=FALSE}

suppressPackageStartupMessages({
  library(dplyr)
  library(kableExtra)
  library(scales)
})

# Base dataset for all trading strategies:
# - requires finite p_market, p_no, p_iron
# - requires non-missing outcome y
trading_base <- iron_predictions_clean %>%
  filter(
    is.finite(p_market),
    is.finite(p_no),
    is.finite(p_iron),
    !is.na(y)
  )

# Baseline Iron Man strategy:
# For each id × stage:
# - If p_iron > p_market: buy 1 YES at p_market
# - If p_iron < p_market: buy 1 NO at p_no
# - Else: no trade
# Hold to resolution.
iron_trading_baseline <- trading_base %>%
  mutate(
    trade = case_when(
      p_iron > p_market ~ "buy_yes",
      p_iron < p_market ~ "buy_no",
      TRUE              ~ "no_trade"
    ),
    trade_price = case_when(
      trade == "buy_yes" ~ p_market,
      trade == "buy_no"  ~ p_no,
      TRUE               ~ NA_real_
    ),
    pnl = case_when(
      trade == "buy_yes" & y == 1 ~ 1 - p_market,
      trade == "buy_yes" & y == 0 ~ -p_market,
      trade == "buy_no"  & y == 0 ~ 1 - p_no,
      trade == "buy_no"  & y == 1 ~ -p_no,
      TRUE                        ~ 0
    )
  )

# -----------------------------
# Stage-level summary
# -----------------------------
iron_baseline_summary_stage <- iron_trading_baseline %>%
  group_by(stage) %>%
  summarise(
    n_rows                = n(),
    n_trades              = sum(trade != "no_trade"),
    n_buy_yes             = sum(trade == "buy_yes"),
    n_buy_no              = sum(trade == "buy_no"),
    total_pnl             = sum(pnl),
    mean_pnl_market       = mean(pnl),  # will be dropped from the final table
    mean_pnl_trade        = if_else(
      n_trades > 0,
      sum(pnl[trade != "no_trade"]) / n_trades,
      NA_real_
    ),
    median_pnl_trade      = if_else(
      n_trades > 0,
      median(pnl[trade != "no_trade"]),
      NA_real_
    ),
    pct_profitable_trades = if_else(
      n_trades > 0,
      mean(pnl[trade != "no_trade"] > 0),
      NA_real_
    ),
    .groups = "drop"
  ) %>%
  mutate(stage = factor(stage, levels = c("first", "mid", "last"))) %>%
  arrange(stage)

# ---- Presentation layer: drop + rename ----
col_labels <- c(
  stage                  = "Stage",
  n_rows                 = "N observations",
  n_trades               = "N trades",
  total_pnl              = "Total PnL",
  mean_pnl_trade         = "Mean PnL (per trade)",
  median_pnl_trade       = "Median PnL (per trade)",
  pct_profitable_trades  = "Share profitable trades"
)
rename_map <- stats::setNames(names(col_labels), col_labels)

iron_baseline_summary_stage_print <- iron_baseline_summary_stage %>%
  select(-n_buy_yes, -n_buy_no, -mean_pnl_market) %>%
  rename(!!!rename_map) %>%
  mutate(`Share profitable trades` = scales::percent(`Share profitable trades`, accuracy = 0.1))

kableExtra::kbl(
  iron_baseline_summary_stage_print,
  digits  = 4,
  caption = "Iron Man baseline YES/NO trading performance by stage (1-share buy-and-hold)."
) %>%
  kableExtra::kable_styling(full_width = FALSE)

# -----------------------------
# Overall summary
# -----------------------------
iron_baseline_summary_overall <- iron_trading_baseline %>%
  summarise(
    n_rows                = n(),
    n_trades              = sum(trade != "no_trade"),
    n_buy_yes             = sum(trade == "buy_yes"),
    n_buy_no              = sum(trade == "buy_no"),
    total_pnl             = sum(pnl),
    mean_pnl_market       = mean(pnl),  # will be dropped from the final table
    mean_pnl_trade        = if_else(
      n_trades > 0,
      sum(pnl[trade != "no_trade"]) / n_trades,
      NA_real_
    ),
    median_pnl_trade      = if_else(
      n_trades > 0,
      median(pnl[trade != "no_trade"]),
      NA_real_
    ),
    pct_profitable_trades = if_else(
      n_trades > 0,
      mean(pnl[trade != "no_trade"] > 0),
      NA_real_
    )
  ) %>%
  mutate(stage = "overall") %>%
  relocate(stage)

iron_baseline_summary_overall_print <- iron_baseline_summary_overall %>%
  select(-n_buy_yes, -n_buy_no, -mean_pnl_market) %>%
  rename(!!!rename_map) %>%
  mutate(`Share profitable trades` = scales::percent(`Share profitable trades`, accuracy = 0.1))

kableExtra::kbl(
  iron_baseline_summary_overall_print,
  digits  = 4,
  caption = "Iron Man baseline YES/NO trading performance (overall)."
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```
***Interpretation** 

These results appear strong, but they must be interpreted with caution.

Because Iron Man is trained in-sample—using the same markets on which it is evaluated—the model has access to information patterns that would not be available to a real trader. This inflates performance and creates an overly optimistic picture of trading profitability.

The high win rate and large positive PnL therefore represent an upper bound on what Iron Man could ever achieve. The out-of-sample analysis in Chapter 8 shows that once this information leakage is removed, the economic performance diminishes.

### 7.4.2 Benchmark: Polymarket Favourite


```{r favourite-baseline-yes-no, message=FALSE, warning=FALSE}
suppressPackageStartupMessages({
  library(dplyr)
  library(kableExtra)
  library(scales)
})

# Polymarket favourite strategy:
# - If p_market > 0.5: buy 1 YES at p_market
# - If p_market < 0.5: buy 1 NO at p_no
# - If exactly 0.5: no trade

favourite_trading <- trading_base %>%
  mutate(
    fav_trade = case_when(
      p_market > 0.5 ~ "buy_yes",
      p_market < 0.5 ~ "buy_no",
      TRUE           ~ "no_trade"
    ),
    fav_price = case_when(
      fav_trade == "buy_yes" ~ p_market,
      fav_trade == "buy_no"  ~ p_no,
      TRUE                   ~ NA_real_
    ),
    fav_pnl = case_when(
      fav_trade == "buy_yes" & y == 1 ~ 1 - p_market,
      fav_trade == "buy_yes" & y == 0 ~ -p_market,
      fav_trade == "buy_no"  & y == 0 ~ 1 - p_no,
      fav_trade == "buy_no"  & y == 1 ~ -p_no,
      TRUE                             ~ 0
    )
  )

# -----------------------------
# Stage-level summary
# -----------------------------
favourite_summary_stage <- favourite_trading %>%
  group_by(stage) %>%
  summarise(
    n_rows                = n(),
    n_trades              = sum(fav_trade != "no_trade"),
    n_buy_yes             = sum(fav_trade == "buy_yes"),
    n_buy_no              = sum(fav_trade == "buy_no"),
    total_pnl             = sum(fav_pnl),
    mean_pnl_market       = mean(fav_pnl),  # will be dropped from the final table
    mean_pnl_trade        = if_else(
      n_trades > 0,
      sum(fav_pnl[fav_trade != "no_trade"]) / n_trades,
      NA_real_
    ),
    median_pnl_trade      = if_else(
      n_trades > 0,
      median(fav_pnl[fav_trade != "no_trade"]),
      NA_real_
    ),
    pct_profitable_trades = if_else(
      n_trades > 0,
      mean(fav_pnl[fav_trade != "no_trade"] > 0),
      NA_real_
    ),
    .groups = "drop"
  ) %>%
  mutate(stage = factor(stage, levels = c("first", "mid", "last"))) %>%
  arrange(stage)

# ---- Presentation layer: drop + rename ----
col_labels <- c(
  stage                  = "Stage",
  n_rows                 = "N observations",
  n_trades               = "N trades",
  total_pnl              = "Total PnL",
  mean_pnl_trade         = "Mean PnL (per trade)",
  median_pnl_trade       = "Median PnL (per trade)",
  pct_profitable_trades  = "Share profitable trades"
)
rename_map <- stats::setNames(names(col_labels), col_labels)

favourite_summary_stage_print <- favourite_summary_stage %>%
  select(-n_buy_yes, -n_buy_no, -mean_pnl_market) %>%
  rename(!!!rename_map) %>%
  mutate(`Share profitable trades` = scales::percent(`Share profitable trades`, accuracy = 0.1))

kableExtra::kbl(
  favourite_summary_stage_print,
  digits  = 4,
  caption = "Polymarket favourite YES/NO trading performance by stage (1-share buy-and-hold)."
) %>%
  kableExtra::kable_styling(full_width = FALSE)

# -----------------------------
# Overall summary
# -----------------------------
favourite_summary_overall <- favourite_trading %>%
  summarise(
    n_rows                = n(),
    n_trades              = sum(fav_trade != "no_trade"),
    n_buy_yes             = sum(fav_trade == "buy_yes"),
    n_buy_no              = sum(fav_trade == "buy_no"),
    total_pnl             = sum(fav_pnl),
    mean_pnl_market       = mean(fav_pnl),  # will be dropped from the final table
    mean_pnl_trade        = if_else(
      n_trades > 0,
      sum(fav_pnl[fav_trade != "no_trade"]) / n_trades,
      NA_real_
    ),
    median_pnl_trade      = if_else(
      n_trades > 0,
      median(fav_pnl[fav_trade != "no_trade"]),
      NA_real_
    ),
    pct_profitable_trades = if_else(
      n_trades > 0,
      mean(fav_pnl[fav_trade != "no_trade"] > 0),
      NA_real_
    )
  ) %>%
  mutate(stage = "overall") %>%
  relocate(stage)

favourite_summary_overall_print <- favourite_summary_overall %>%
  select(-n_buy_yes, -n_buy_no, -mean_pnl_market) %>%
  rename(!!!rename_map) %>%
  mutate(`Share profitable trades` = scales::percent(`Share profitable trades`, accuracy = 0.1))

kableExtra::kbl(
  favourite_summary_overall_print,
  digits  = 4,
  caption = "Polymarket favourite YES/NO trading performance (overall)."
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

### 7.3.3 Comparison: Iron Man vs Market Favourite

```{r iron-vs-favourite-comparison, message=FALSE, warning=FALSE}
# Stage-level comparison
comparison_stage <- iron_baseline_summary_stage %>%
  select(
    stage,
    iron_total_pnl      = total_pnl,
    iron_mean_pnl_trade = mean_pnl_trade
  ) %>%
  inner_join(
    favourite_summary_stage %>%
      select(
        stage,
        fav_total_pnl      = total_pnl,
        fav_mean_pnl_trade = mean_pnl_trade
      ),
    by = "stage"
  ) %>%
  mutate(
    diff_total_pnl      = iron_total_pnl - fav_total_pnl,
    diff_mean_pnl_trade = iron_mean_pnl_trade - fav_mean_pnl_trade
  )

kableExtra::kbl(
  comparison_stage,
  digits  = 4,
  caption = "Stage-level PnL comparison: Iron Man vs Polymarket favourite"
) %>%
  kableExtra::kable_styling(full_width = FALSE)

# Overall comparison
comparison_overall <- bind_rows(
  iron_baseline_summary_overall %>%
    mutate(strategy = "Iron Man"),
  favourite_summary_overall %>%
    mutate(strategy = "Market favourite")
) %>%
  select(
    strategy,
    n_rows,
    n_trades,
    n_buy_yes,
    n_buy_no,
    total_pnl,
    mean_pnl_market,
    mean_pnl_trade,
    median_pnl_trade,
    pct_profitable_trades
  )

kableExtra::kbl(
  comparison_overall,
  digits  = 4,
  caption = "Overall trading performance: Iron Man vs Polymarket favourite"
) %>%
  kableExtra::kable_styling(full_width = FALSE)

```
```{r iron-vs-Iron Man OOS - vs-favourite-comparison, message=FALSE, warning=FALSE}
# -----------------------------
# Overall comparison: Iron Man (in-sample) vs Iron Man OOS vs Market favourite
# -----------------------------

comparison_overall <- dplyr::bind_rows(
  iron_baseline_summary_overall %>%
    dplyr::mutate(strategy = "Iron Man"),
  iron_cv_summary_overall %>%
    dplyr::mutate(strategy = "Iron Man OOS"),
  favourite_summary_overall %>%
    dplyr::mutate(strategy = "Market favourite")
) %>%
  dplyr::select(
    strategy,
    n_rows,
    n_trades,
    total_pnl,
    mean_pnl_trade,
    median_pnl_trade,
    pct_profitable_trades
  ) %>%
  dplyr::mutate(
    strategy = factor(strategy, levels = c("Iron Man", "Iron Man OOS", "Market favourite"))
  ) %>%
  dplyr::arrange(strategy)

# ---- Presentation layer: rename columns ----
col_labels <- c(
  strategy              = "Strategy",
  n_rows                = "N observations",
  n_trades              = "N trades",
  total_pnl             = "Total PnL",
  mean_pnl_trade        = "Mean PnL (per trade)",
  median_pnl_trade      = "Median PnL (per trade)",
  pct_profitable_trades = "Share profitable trades"
)
rename_map <- stats::setNames(names(col_labels), col_labels)

comparison_overall_print <- comparison_overall %>%
  dplyr::rename(!!!rename_map) %>%
  dplyr::mutate(
    `Share profitable trades` = scales::percent(`Share profitable trades`, accuracy = 0.1)
  )

kableExtra::kbl(
  comparison_overall_print,
  digits  = 4,
  caption = "Overall trading performance: Iron Man (in-sample) vs Iron Man (out-of-sample CV) vs Polymarket favourite."
) %>%
  kableExtra::kable_styling(full_width = FALSE)

```
Interpretation for subsection 7.3.3 :


Iron Man does achieve a higher mean PnL than the market favourite (0.098 vs 0.028 per trade), but this advantage is likely driven by the use of in-sample outcomes during training. The model is effectively allowed to learn patterns that would be unavailable in any real forecasting or trading environment. Once this is recognized, the difference loses economic relevance.

The overall magnitude of the profits is the more important indicator. The median trade profit for Iron Man is only 0.019, which is not far from the market favourite’s median (0.016). The market favorite produces a higher fraction of winning trades (95%), this indicates that Polymarket’s prices already embed almost all of the tradable signal. Polymarket prices remain highly efficient relative to the information contained in the tested LLM forecasts.


<!-- ### 7.4.1 Polymarket performance fee on winning trades Take this out, I am going a diffrent way now, lets simplify and go straight up to the K fold  -->

<!-- ```{r winner-fee-only, message=FALSE, warning=FALSE} -->
<!-- library(dplyr) -->
<!-- library(kableExtra) -->

<!-- # Polymarket performance fee: ~2% of net profit on winning bets -->
<!-- performance_fee <- 0.02 -->

<!-- # Iron Man: apply 2% fee only to positive PnL trades -->
<!-- iron_fee_results <- iron_trading_baseline %>% -->
<!--   mutate( -->
<!--     fee      = performance_fee * pmax(pnl, 0),   # 0 for losing or zero PnL -->
<!--     pnl_net  = pnl - fee -->
<!--   ) %>% -->
<!--   summarise( -->
<!--     strategy              = "Iron Man", -->
<!--     n_rows                = n(), -->
<!--     n_trades              = sum(trade != "no_trade"), -->
<!--     total_pnl_gross       = sum(pnl), -->
<!--     total_fee             = sum(fee), -->
<!--     total_pnl_net         = sum(pnl_net), -->
<!--     mean_pnl_trade_gross  = total_pnl_gross / n_trades, -->
<!--     mean_pnl_trade_net    = total_pnl_net / n_trades -->
<!--   ) -->

<!-- # Market favourite: same logic -->
<!-- favourite_fee_results <- favourite_trading %>% -->
<!--   mutate( -->
<!--     fee      = performance_fee * pmax(fav_pnl, 0), -->
<!--     pnl_net  = fav_pnl - fee -->
<!--   ) %>% -->
<!--   summarise( -->
<!--     strategy              = "Market favourite", -->
<!--     n_rows                = n(), -->
<!--     n_trades              = sum(fav_trade != "no_trade"), -->
<!--     total_pnl_gross       = sum(fav_pnl), -->
<!--     total_fee             = sum(fee), -->
<!--     total_pnl_net         = sum(pnl_net), -->
<!--     mean_pnl_trade_gross  = total_pnl_gross / n_trades, -->
<!--     mean_pnl_trade_net    = total_pnl_net / n_trades -->
<!--   ) -->

<!-- winner_fee_table <- bind_rows(iron_fee_results, favourite_fee_results) %>% -->
<!--   select( -->
<!--     strategy, -->
<!--     n_rows, n_trades, -->
<!--     total_pnl_gross, total_fee, total_pnl_net, -->
<!--     mean_pnl_trade_gross, mean_pnl_trade_net -->
<!--   ) -->

<!-- kbl( -->
<!--   winner_fee_table, -->
<!--   digits  = 4, -->
<!--   caption = "Effect of a 2% performance fee on winning trades (Polymarket-style) for Iron Man and the market favourite" -->
<!-- ) %>% -->
<!--   kable_styling(full_width = FALSE) -->
<!-- ``` -->


# Chapter 8 Out of sample trading

## 8.1 Motivation

The trading results in Chapter 7 represent a best-case scenario: Iron Man is trained on the same markets it trades, which inflates both its calibration and its trading performance. In practice, a trader only has access to historical markets when forecasting a new one.

To assess whether the Iron Man model has real economic value, this chapter implements cross-validated, out-of-sample Iron Man probabilities, restricting the model so that it cannot access the market it is predicting. These out-of-sample probabilities are then used to re-run the trading simulations from Chapter 7.

## 8.2 Creating cross validated Iron man predictions

To evaluate Iron Man under realistic forecasting conditions, each market must be predicted without using its own outcome for training. I therefore construct p_iron_cv, an out-of-sample probability obtained through 5-fold cross-validation across markets. In each fold, the model is trained on four-fifths of the markets and used to predict the remaining fifth, ensuring that every probability is generated strictly out-of-sample. This approach removes in-sample leakage and provides a more credible measure of Iron Man’s true predictive performance.

```{r ironman-kfold-cv, message=FALSE, warning=FALSE}
library(dplyr)
library(purrr)
library(tidyr)
library(stringr)

set.seed(123)  # reproducible CV folds

# Start from the cleaned iron_data from earlier chapters
iron_data_input <- iron_data  # rename for clarity

# 1. Assign each market id to a CV fold (5-fold CV)
ids <- iron_data_input %>%
  distinct(id) %>%
  arrange(id) %>%
  mutate(fold = sample(rep(1:5, length.out = n())))

iron_data_folds <- iron_data_input %>%
  left_join(ids, by = "id")

# 2. Predictors for Iron Man
predictor_cols <- c(
  "p_market",
  names(iron_data_folds) %>% str_subset("(zs|cot)$")
)

formula_iron <- as.formula(
  paste("y ~", paste(predictor_cols, collapse = " + "))
)

# 3. Cross-validated predictions per stage
cv_results <- iron_data_folds %>%
  group_by(stage) %>%
  group_modify(~{
    stage_df <- .x
    p_cv <- rep(NA_real_, nrow(stage_df))
# for every stage k tain on the other stages and predict for k
    for (k in sort(unique(stage_df$fold))) {
      train_df <- stage_df %>% filter(fold != k)
      test_df  <- stage_df %>% filter(fold == k)

      model_k <- glm(
        formula_iron,
        data = train_df,
        family = binomial(link = "logit")
      )

      p_k <- predict(model_k, newdata = test_df, type = "response")
      p_cv[stage_df$fold == k] <- p_k
    }

    stage_df %>% mutate(p_iron_cv = p_cv)
  }) %>%
  ungroup()

iron_data_cv <- cv_results

# sanity check
iron_data_cv %>%
  summarise(
    n_rows = n(),
    n_missing_p_cv = sum(is.na(p_iron_cv))
  )
```
## 8.3 Prepare cross-validated trading dataset

Before running the OOS trading simulation, we must align the data with the structure used in Chapter 7:

- Ensure `p_no` exists for all rows (insert `1 - p_market` where missing)  
- Keep only rows with valid outcome `y`



```{r ironman-cv-predictions, message=FALSE}
# ===============================================================
# 8.3: Prepare dataset for OOS trading simulation
# ===============================================================
# This mirrors the preprocessing in Chapter 7:
#   - Handle missing p_no
#   - Ensure we have clean rows with outcome y and prices
#   - Output: iron_predictions_cv
# ===============================================================

iron_predictions_cv <- iron_data_cv %>%
  mutate(
    p_no_missing_raw = is.na(p_no),
    # If NO price is missing, infer from YES price (valid for binary markets)
    p_no = if_else(
      is.na(p_no) & !is.na(p_market),
      1 - p_market,
      p_no
    )
  )

# Sanity check: all good?
iron_predictions_cv %>%
  summarise(
    n_rows         = n(),
    n_missing_p_cv = sum(is.na(p_iron_cv)),
    n_missing_p_no = sum(is.na(p_no)),
    n_missing_y    = sum(is.na(y))
  )
```
## 8.4 Out-of-sample Iron Man trading

Now we run the exact same trading rules as in Chapter 7, but using the **cross-validated p_iron_cv** instead of the in-sample p_iron.

This finally answers the key question:

 Does Iron Man still generate trading profits when it is not allowed to fit the markets it trades?

```{r ironman-longonly-sim-cv, message=FALSE, warning=FALSE}
suppressPackageStartupMessages({
  library(dplyr)
  library(kableExtra)
  library(scales)
})

# Align with Chapter 7 filtering criteria
trading_base_cv <- iron_predictions_cv %>%
  filter(
    is.finite(p_market),
    is.finite(p_no),
    is.finite(p_iron_cv),
    !is.na(y)
  )

# Compute trades and PnL
iron_trading_cv <- trading_base_cv %>%
  mutate(
    trade = case_when(
      p_iron_cv > p_market ~ "buy_yes",
      p_iron_cv < p_market ~ "buy_no",
      TRUE                 ~ "no_trade"
    ),
    trade_price = case_when(
      trade == "buy_yes" ~ p_market,
      trade == "buy_no"  ~ p_no,
      TRUE               ~ NA_real_
    ),
    pnl = case_when(
      trade == "buy_yes" & y == 1 ~ 1 - p_market,
      trade == "buy_yes" & y == 0 ~ -p_market,
      trade == "buy_no"  & y == 0 ~ 1 - p_no,
      trade == "buy_no"  & y == 1 ~ -p_no,
      TRUE                        ~ 0
    )
  )

# -----------------------------
# Stage-level summary
# -----------------------------
iron_cv_summary_stage <- iron_trading_cv %>%
  group_by(stage) %>%
  summarise(
    n_rows                = n(),
    n_trades              = sum(trade != "no_trade"),
    total_pnl             = sum(pnl),
    mean_pnl_trade        = if_else(
      n_trades > 0,
      sum(pnl[trade != "no_trade"]) / n_trades,
      NA_real_
    ),
    median_pnl_trade      = if_else(
      n_trades > 0,
      median(pnl[trade != "no_trade"]),
      NA_real_
    ),
    pct_profitable_trades = if_else(
      n_trades > 0,
      mean(pnl[trade != "no_trade"] > 0),
      NA_real_
    ),
    .groups = "drop"
  ) %>%
  mutate(stage = factor(stage, levels = c("first", "mid", "last"))) %>%
  arrange(stage)

# ---- Presentation layer: rename ----
col_labels <- c(
  stage                 = "Stage",
  n_rows                = "N observations",
  n_trades              = "N trades",
  total_pnl             = "Total PnL",
  mean_pnl_trade        = "Mean PnL (per trade)",
  median_pnl_trade      = "Median PnL (per trade)",
  pct_profitable_trades = "Share profitable trades"
)
rename_map <- stats::setNames(names(col_labels), col_labels)

iron_cv_summary_stage_print <- iron_cv_summary_stage %>%
  rename(!!!rename_map) %>%
  mutate(`Share profitable trades` = scales::percent(`Share profitable trades`, accuracy = 0.1))

kableExtra::kbl(
  iron_cv_summary_stage_print,
  digits = 4,
  caption = "Out-of-sample Iron Man trading performance by stage (5-fold CV)."
) %>%
  kableExtra::kable_styling(full_width = FALSE)

# -----------------------------
# Overall summary
# -----------------------------
iron_cv_summary_overall <- iron_trading_cv %>%
  summarise(
    n_rows                = n(),
    n_trades              = sum(trade != "no_trade"),
    total_pnl             = sum(pnl),
    mean_pnl_trade        = if_else(
      n_trades > 0,
      sum(pnl[trade != "no_trade"]) / n_trades,
      NA_real_
    ),
    median_pnl_trade      = if_else(
      n_trades > 0,
      median(pnl[trade != "no_trade"]),
      NA_real_
    ),
    pct_profitable_trades = if_else(
      n_trades > 0,
      mean(pnl[trade != "no_trade"] > 0),
      NA_real_
    )
  ) %>%
  mutate(stage = "overall") %>%
  relocate(stage)

iron_cv_summary_overall_print <- iron_cv_summary_overall %>%
  rename(!!!rename_map) %>%
  mutate(`Share profitable trades` = scales::percent(`Share profitable trades`, accuracy = 0.1))

kableExtra::kbl(
  iron_cv_summary_overall_print,
  digits = 4,
  caption = "Out-of-sample Iron Man trading performance (overall, 5-fold CV)."
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

## 8.5 Comparison against the Polymarket Favourite

The final step in evaluating the economic usefulness of the Iron Man model is to benchmark it against a very simple trading rule: always buy the Polymarket favorite (the contract priced above 0.5).

This baseline requires no modelling, no LLMs, and no historical fitting. If Iron Man cannot outperform this trivial rule under realistic out-of-sample conditions, it becomes obvious that bothering with LLM implementations for forecasting with the resource cost associated with it, is non sensible and economically unwise.

The table below compares out-of-sample Iron Man trading performance against the Polymarket favorite across all 159 market-stage observations.

```{r polymarket-favourite-oos, message=FALSE, warning=FALSE}
# ============================================================
# 8.5: Compare OOS Iron Man vs Polymarket Favourite
# ============================================================

# Reuse the same base dataset as in 8.4
trading_base_cv <- iron_predictions_cv %>%
  filter(
    is.finite(p_market),
    is.finite(p_no),
    is.finite(p_iron_cv),
    !is.na(y)
  )

# Polymarket favourite strategy
fav_trading <- trading_base_cv %>%
  mutate(
    fav_trade = case_when(
      p_market > 0.5 ~ "buy_yes",
      p_market < 0.5 ~ "buy_no",
      TRUE           ~ "no_trade"
    ),
    fav_price = case_when(
      fav_trade == "buy_yes" ~ p_market,
      fav_trade == "buy_no"  ~ p_no,
      TRUE                   ~ NA_real_
    ),
    #the pay out system
    fav_pnl = case_when(
      fav_trade == "buy_yes" & y == 1 ~ 1 - p_market,
      fav_trade == "buy_yes" & y == 0 ~ -p_market,
      fav_trade == "buy_no"  & y == 0 ~ 1 - p_no,
      fav_trade == "buy_no"  & y == 1 ~ -p_no,
      TRUE                            ~ 0
    )
  )

# Aggregate comparison table (overall)
comparison_overall <- tibble(
  strategy = c("Iron Man (OOS)", "Market favourite"),
  total_pnl = c(
    sum(iron_trading_cv$pnl),
    sum(fav_trading$fav_pnl)
  ),
  mean_pnl_trade = c(
    mean(iron_trading_cv$pnl),
    mean(fav_trading$fav_pnl)
  ),
  median_pnl_trade = c(
    median(iron_trading_cv$pnl),
    median(fav_trading$fav_pnl)
  ),
  pct_profitable = c(
    mean(iron_trading_cv$pnl > 0),
    mean(fav_trading$fav_pnl > 0)
  )
)

comparison_overall %>%
  kbl(digits = 4,
      caption = "Out-of-sample trading performance: Iron Man vs Polymarket favourite") %>%
  kable_styling(full_width = FALSE)
```

***Interpretation***

The comparison is decisive. Across all markets and stages, the Polymarket favourite strategy outperforms the out-of-sample Iron Man model on every metric.

First, the total PnL of the favourite strategy (4.38) is nearly four times larger than Iron Man’s (1.10). The gap is economically meaningful: a trader allocating 1 unit per trade would earn 4.4 units by simply backing the favorite, whereas Iron Man yields only 1.1 units.

Second, the average profit per trade is substantially higher for the favorite rule (0.0276) compared to Iron Man (0.0069). This indicates that Iron Man’s forecasts offer only marginal improvements over random noise once evaluated out-of-sample.

Third, the median profit per trade is consistently higher for the favorite strategy (0.016 versus 0.009), confirming that the favourite rule delivers more stable outcomes rather than relying on a few large wins.

Most strikingly, the share of profitable trades is 95.6% for the favorite strategy, compared to only 67.3% for Iron Man. A strategy that loses on one-third of trades is not competitive with one that almost never loses.

Taken together, the evidence shows that the marginal statistical accuracy improvements provided by Iron Man do not translate into economic value. Under realistic out-of-sample constraints, Iron Man is strictly dominated by the simplest possible benchmark. There is no scenario in which a rational trader would prefer Iron Man over just buying the favorite.





